{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "revision machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import gzip\n",
    "import random\n",
    "import secrets\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "import plotly.io as pio\n",
    "from sklearn import tree\n",
    "import plotly.express as px\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from flask import Flask, request, render_template, session, url_for, redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Salaire  Diplome  Annee_naissance  Anciennete\n",
      "0  156080250   Master             1997          39\n",
      "1  185340196   Master             1962           9\n",
      "2    6422753  Licence             1969          30\n",
      "3  149236268      Bac             1950          20\n",
      "4  125787327   Master             1978          29\n"
     ]
    }
   ],
   "source": [
    "# Création des colonnes\n",
    "salaires = np.random.randint(1500, 200000001, size=200)\n",
    "diplomes = [\"Bac\", \"Licence\", \"Master\", \"Doctorat\"]\n",
    "diplomes = np.random.choice(diplomes, size=200)\n",
    "Annee_naissance = np.random.randint(1950, 2005, size=200)\n",
    "Anciennete = np.random.randint(1, 50, size=200)\n",
    "\n",
    "\n",
    "# Créer un dataframe à partir des tableaux numpy\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Salaire\": salaires,\n",
    "        \"Diplome\": diplomes,\n",
    "        \"Annee_naissance\": Annee_naissance,\n",
    "        \"Anciennete\": Anciennete,\n",
    "    }\n",
    ")\n",
    "# Afficher les premières lignes du DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de valeurs uniques\n",
    "n_unique_categories = df.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Salaire            200\n",
       "Annee_naissance     55\n",
       "Anciennete          49\n",
       "Diplome              4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"name_basics\"].dropna(subset=(\"birthYear\", \"deathYear\"))\n",
    "df[\"name_basics\"].dropna(axis=0, thresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatene colonne avec numpy\n",
    "np.concatenate([\"col1\", \"col2\"])\n",
    "\n",
    "# concatene colonne avec pandas\n",
    "# il faut d'abord aplatir les dataframes\n",
    "df[\"col1\"]\n",
    "np.ndarray.flatten(\"col1\")\n",
    "np.ndarray.flatten(\"col2\")\n",
    "ser1 = pd.Series(\"col1\")\n",
    "ser2 = pd.Series(\"col2\")\n",
    "pd.concat([ser1, ser2], axis=1, names=[\"ser1\", \"ser2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"name_basics\"][\"birthYear\"].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"name_basics\"].groupby(\"birthYear\").dropna(axis=0, thresh=1)\n",
    "dfs[\"name_basics\"].groupby(\"birthYear\").mean()\n",
    "dfs[\"name_basics\"].groupby(\"birthYear\").mean()\n",
    "\n",
    "dfs[\"name_basics\"].groupby(\"birthYear\").sum()\n",
    "dfs[\"name_basics\"].groupby(\"birthYear\").describe()\n",
    "dfs[\"name_basics\"].groupby(\"birthYear\").mean().add_prefix(\"année naissance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointure df1 et df2 automatique par colonne s'il y a au moins 1 colonne en commun\n",
    "df3 = pd.merge(df1, df2)\n",
    "\n",
    "# si plrs colonnes en commun pour lui imposer une colonne et ainsi eviter de mulitplier les lignes\n",
    "pd.merge(df3, df4, left_on=\"salarie\", right_on=\"Nom\", how=\"inner\")\n",
    "\n",
    "# avec join\n",
    "pd.join(df3, df4, left_on=\"salarie\", right_on=\"Nom\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode\n",
    "dF.explode(\"knownForTitles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage ici donnera le  meme chiffre pour une même diplome\n",
    "pd.get_dummies(df3[\"Diplome\"])\n",
    "# autre solution avec map\n",
    "df3[\"Diplome\"].map({\"ingenieur\": 1, \"technicien\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Utiliser LabelEncoder pour encoder les colonnes\n",
    "le = LabelEncoder()\n",
    "df[\"Diplome\"] = le.fit_transform(df[\"Diplome\"])\n",
    "df[\"Salaire\"] = le.fit_transform(df[\"Salaire\"])\n",
    "df[\"Annee_naissance\"] = le.fit_transform(df[\"Annee_naissance\"])\n",
    "# Normaliser les données avec StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "# Utiliser l'ACP pour réduire la dimensionnalité\n",
    "pca = PCA(n_components=1)\n",
    "df_pca = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live coding Patrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(0, 2, 0.2)\n",
    "ser = pd.Series(r)\n",
    "ser\n",
    "ser.sum()\n",
    "ser.min()\n",
    "ser.max()\n",
    "ser.describe()\n",
    "df1 = pd.DataFrame({\"A\": ser[:4] + 1, \"B\": ser[4:] + 2})\n",
    "df1\n",
    "df1.isna()\n",
    "df1.A.isna()\n",
    "df1.A.isnull()\n",
    "df1[df1.A.isna()]\n",
    "df1.A.notna()\n",
    "df1.A.notnull()\n",
    "df1.dropna(axis=0)\n",
    "df1.dropna(axis=0, thresh=1)\n",
    "df1.fillna(0)\n",
    "df1.fillna({\"A\": 1, \"B\": 2})\n",
    "df1.fillna({\"A\": df1.A.mean(), \"B\": df1.B.max()})\n",
    "df1\n",
    "# df1 = df1.fillna({\"A\" : df1.A.mean(), \"B\" : df1.B.max()})\n",
    "df1.fillna({\"A\": df1.A.mean(), \"B\": df1.B.max()}, inplace=True)\n",
    "df1\n",
    "df1.mean(axis=1)\n",
    "df1.mean(axis=0)\n",
    "df2 = pd.DataFrame(\n",
    "    {\"étudiant\": [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"], \"notes\": np.random.randint(12, 18, 6)},\n",
    "    columns=[\"étudiant\", \"notes\"],\n",
    ")\n",
    "df2\n",
    "df2.groupby(\"étudiant\").sum()\n",
    "df2.groupby(\"étudiant\").mean()\n",
    "df2.groupby(\"étudiant\")\n",
    "df2.groupby(\"étudiant\").describe()\n",
    "df2.groupby(\"étudiant\").mean().add_prefix(\"moyenne générale_\")\n",
    "ar1 = np.arange(2, 20, 2).reshape(3, 3)\n",
    "ar2 = ar1 + round(np.random.randint(0, 10) + np.pi, 2)\n",
    "round(np.random.randint(0, 10) + np.pi, 2)\n",
    "ar1\n",
    "ar2\n",
    "np.concatenate([ar1, ar2])\n",
    "np.ndarray.flatten(ar1)\n",
    "ser1 = pd.Series(np.ndarray.flatten(ar1))\n",
    "ser2 = pd.Series(np.ndarray.flatten(ar2))\n",
    "pd.concat([ser1, ser2], axis=0)\n",
    "pd.concat([ser1, ser2], axis=1, names=[\"ser1\", \"ser2\"])\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"Salarié\": [\"Eric\", \"Hapsa\", \"Elise\", \"Xin\"],\n",
    "        \"groupe\": [\"Technicien\", \"Ingénieur\", \"Ingénieur\", \"Technicien\"],\n",
    "    }\n",
    ")\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Salarié\": [\"Hapsa\", \"Elise\", \"Eric\", \"Xin\"],\n",
    "        \"date_embauche\": [2001, 1988, 2012, 2016],\n",
    "    }\n",
    ")\n",
    "df3 = pd.merge(df1, df2)\n",
    "df3\n",
    "df4 = pd.DataFrame(\n",
    "    {\"groupe\": [\"Technicien\", \"Ingénieur\"], \"Superviseur\": [\"Carlos\", \"Amine\"]}\n",
    ")\n",
    "df4\n",
    "df4 = pd.merge(df3, df4)\n",
    "df4\n",
    "df5 = pd.DataFrame(\n",
    "    {\n",
    "        \"groupe\": [\"Technicien\", \"Technicien\", \"Ingénieur\", \"Ingénieur\", \"Ingénieur\"],\n",
    "        \"compétences\": [\n",
    "            \"tableurs\",\n",
    "            \"maintenance\",\n",
    "            \"codage\",\n",
    "            \"conduite de changement\",\n",
    "            \"recherche opérationelle\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df5\n",
    "pd.merge(df4, df5)\n",
    "df6 = pd.DataFrame(\n",
    "    {\n",
    "        \"Nom\": [\"Hapsa\", \"Elise\", \"Xin\", \"Amine\"],\n",
    "        \"salaire\": [90000, 100000, 80000, 100000],\n",
    "    }\n",
    ")\n",
    "df6\n",
    "df3\n",
    "pd.merge(df3, df6, left_on=\"Salarié\", right_on=\"Nom\", how=\"inner\")\n",
    "df7 = pd.merge(df3, df6, left_on=\"Salarié\", right_on=\"Nom\", how=\"inner\")\n",
    "df7\n",
    "pd.get_dummies(df7.groupe)\n",
    "df7.groupe.map({\"Ingénieur\": 1, \"Technicien\": 0})\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LabelEncoder().fit_transform(df7.groupe)\n",
    "df6.describe()\n",
    "df8 = pd.DataFrame(\n",
    "    {\n",
    "        \"Nom\": [\"Hapsa\", \"Elise\", \"Xin\", \"Amine\", \"Mpabé\"],\n",
    "        \"salaire\": [90000, 100000, 80000, 100000, 72000000],\n",
    "    }\n",
    ")\n",
    "df8.describe().mean()  # 14 875 250\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "StandardScaler().fit_transform(df8.salaire.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisé > faire en sorte que la moyenne soit égale à 0 et que la variance soit égale à 1\n",
    "\n",
    "# centrage  > soustraire à toutes ces observations leur moyenne\n",
    "centrage = df8.salaire - df8.salaire.mean()\n",
    "\n",
    "# réduction > on les divise par leur écart-type\n",
    "reduction = centrage / df8.salaire.std()  # centrage.std() #\n",
    "\n",
    "reduction\n",
    "df2\n",
    "df10 = pd.DataFrame(\n",
    "    {\n",
    "        \"Brand\": [\"Ford\", \"Ford\", \"Ford\"],\n",
    "        \"Model\": [\"Sierra\", \"F-150\", \"Mustang\"],\n",
    "        \"Typ\": [\"2.0 GL\", \"Raptor\", [\"Mach-E\", \"Mach-1\", \"Shelby\"]],\n",
    "    }\n",
    ")\n",
    "df10\n",
    "df10.explode(\"Typ\")\n",
    "gauss = np.random.randn(10000)\n",
    "plt.hist(gauss, bins=100, color=\"red\")\n",
    "plt.grid()\n",
    "plt.title(\"Données normalement distribuées\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des données normalement distribuées (qui suivent une distribution gaussienne) centré (la moyenne est à 0) et réduite (l'écart-type vaut 1) ont comme caractéristiques :\n",
    "- environ 68 % des valeurs qui vont être tirées vont être comprises entre zéro plus un écart-type et zéro moins un écart-type\n",
    "- 95 % environ des valeurs qui vont être tirées ou bien qui vont faire partie d'un échantillon issu d'une population qui suit une loi normale vont être comprises entre zéro plus deux écarts-type et zéro moins deux écarts-type\n",
    "- environ 99,7 % des valeurs d'un échantillon issus d'une population qui suit une loi normale vont être comprises entre zéro plus trois écarts-type et zéro moins trois écarts-type\n",
    "\n",
    "Si on tire d'une population qui suit une loi normale un échantillon représentatif, cet échantillon suivra une loi normale également, avec les caractéristiques associées. Par contre, si la population suit une autre loi, autre que la loi normale, alors, pour que le théorème central limite soit applicable, il faudrait que la taille des échantillons soit au minimum égale à 30, Scikit learn préférant une marge minimale à 50 individus pour faire l'approximation d'une loi normale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Étapes d'un projet médical > mise en vente d'un nouveau médicaments**\n",
    "- Il y aura des effets secondaires > assurance du fabriquant\n",
    "- Il faut que ce médicament est un *résultat* supérieur à l'effet placebo > à démontrer\n",
    "\n",
    "> Score d'efficacité de 94% sur le train et 92% sur le test\n",
    "- Est-ce que les donneés étaient normalement distribué ? > Permet de rendre un modèle plus robuste\n",
    "- Test d'indépendance des variables > Test exact de Fisher > Parce que sinon le modèle prédit la même chose que ses features\n",
    "- Test de colinéarité > (vitesse en KM pour prédire la vitesse en Miles)\n",
    "- Test pour valider les hypothèses d'utilisation d'un modèle >\n",
    "  - Homoscédasticité > lorsque la variance des erreurs stochastiques (aléatoires) de la régression est la même pour chaque observation i (de 1 à n observations).\n",
    "  - stationnarité > Dickey-Fuller\n",
    "  - Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/c/c9/Tableau_hypoth%C3%A8ses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypothèse H0 > statut quo\n",
    "- Hypothèse H1 > à démontrer\n",
    "\n",
    "p_value > une valeur de vraisemble du résultat du test statistique. Si la p_value est supérieur au seuil conventionnel de 0.05 avec l'hypothèse H1 est refusé et l'hypothèse H0 est accepté. Si la p_value est inférieur au seuil 0.05, l'hypothèse H1 est accepté, et H0 est refusé.\n",
    "\n",
    "Exemple :     \n",
    "- La p_value pour une personne qui a gagné au loto 3 fois d'affilé sera très grande, ce qui indique une grande improbabilité, et donc qu'il a triché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 > Données normalement distribuées, p_value > 0.05\n",
    "# H1 > Données pas normalement distribuées, p_value < 0.05\n",
    "stats.normaltest(gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Étapes de réalisation d'un projet Data Science pro**\n",
    "- Déterminer si on fait de l'apprentissage supervisé ou non supervisé\n",
    "- Déterminer si on fait de la régression ou de al classification\n",
    "- EDA :\n",
    "  - Analyse descriptive des données : stats (tendance centrale, forme (kustosis, skewness)...), **nettoyage**, valeurs manquantes (fillna, dropna, isna..), outliers (boxplot, ecart intercartille, z-score)\n",
    "  - **Analyse exploratoire** : corrélation linéaire (quanti-quanti), Chi2 (quali-quali), anova (quanti-quali), heatmap, pairplot,\n",
    "  - Test statistiques : normalité, homoscédasticité, colinéarité, stationarité, analyse de la p_value...\n",
    "- Preprocessing :\n",
    "  - encodage : dummies, onehot...\n",
    "  - standardisation : min_max, centrage_reduction...\n",
    "  - reduction de dimenssion : acp et analyse du cercle de corrélation\n",
    "  - Les pré-requis des modèles à utiliser\n",
    "- Modélisation :     \n",
    "  - Choix d'une liste de modèle qu'on classe par complexité (du moins au plus)\n",
    "  - grid search\n",
    "  - train_test split avec KFold (pour vérifier l'impact de la distribution des splits sur le résultats)\n",
    "  - Choix des métriques d'évaluation des modèles\n",
    "  - Clustering sur les données pour un score plus adapté\n",
    "  - pipeline pour automatiser les étapes précédantes\n",
    "- Monitoring du modèle : évaluation au cours du temps de l'amélioration ou de la dégradation des performances, et potentionnel changement de modèle\n",
    "- Intégration dans une application user friendly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
