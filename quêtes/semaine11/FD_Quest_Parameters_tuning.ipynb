{"cells":[{"cell_type":"markdown","metadata":{"id":"tLTWTTh057Bz"},"source":["### The XOR-Model ###\n","\n","We will build our tiny neural network predicting the XOR-data in keras\n","\n","First of all, we need to import the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"p9mkaLawe9zf"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"]},{"cell_type":"markdown","metadata":{"id":"7eYcdsPse4f2"},"source":["We need to define simple datas.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yo7zQOL-6sOL"},"outputs":[],"source":["#Input datasets\n","x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y_train = np.array([[0],[1],[1],[0]])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_QPZyiW56zoW"},"source":["Now we set up our parameters - and define the model, exactly as in the lecture slides\n","\n","TODO: Show different way to set up the model, with and without Input layer"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"RwYszp9qoxzh"},"outputs":[{"data":{"text/plain":["(1, 4)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Set up your model here :\n","x_train.shape\n","y_train.flatten().shape\n","y_train.reshape(1, -1).shape\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","model = Sequential()\n","model.add(Dense(500, activation='relu', input_dim=2))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(50, activation=\"relu\"))\n","model.add(Dense(2, activation=\"softmax\"))\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"]}],"source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=[\"accuracy\"])\n"]},{"cell_type":"markdown","metadata":{"id":"fMBXTEQg7Ejx"},"source":["We already have compiled our model - now we need to train it. We also need to get some predictions in order to see whether our model can indeed predict the XOR-data"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"sbDOj8JW7JTV"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n","WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","1/1 [==============================] - 1s 1s/step - loss: 0.7002 - accuracy: 0.5000\n","Epoch 2/10\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6763 - accuracy: 0.5000\n","Epoch 3/10\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6615 - accuracy: 0.7500\n","Epoch 4/10\n","1/1 [==============================] - 0s 5ms/step - loss: 0.6478 - accuracy: 0.7500\n","Epoch 5/10\n","1/1 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 1.0000\n","Epoch 6/10\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6235 - accuracy: 1.0000\n","Epoch 7/10\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6110 - accuracy: 1.0000\n","Epoch 8/10\n","1/1 [==============================] - 0s 5ms/step - loss: 0.5973 - accuracy: 1.0000\n","Epoch 9/10\n","1/1 [==============================] - 0s 6ms/step - loss: 0.5846 - accuracy: 1.0000\n","Epoch 10/10\n","1/1 [==============================] - 0s 5ms/step - loss: 0.5708 - accuracy: 1.0000\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x220d2f54fd0>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Train your model here, and predict the XOR-data :\n","model.fit(x_train, y_train.flatten(), epochs=10)\n"]},{"cell_type":"markdown","metadata":{"id":"Ojlc0RHB6vXA"},"source":["What is wrong with these predictions?\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"cOlTwqPMgOQk"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 205ms/step\n"]},{"data":{"text/plain":["array([[0.5045543 , 0.49544576],\n","       [0.40081057, 0.59918934],\n","       [0.38915473, 0.6108453 ],\n","       [0.58468777, 0.41531223]], dtype=float32)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Please display predicted values :\n","model.predict(x_train)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 94ms/step\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predictions</th>\n","      <th>probabilités</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.504554</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.599189</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0.610845</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.584688</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   predictions  probabilités\n","0            0      0.504554\n","1            1      0.599189\n","2            1      0.610845\n","3            0      0.584688"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","predictionsFinale = []\n","for ligne in model.predict(x_train):\n","  if ligne[0] > ligne[1]:\n","    predictionsFinale.append([0, ligne[0]])\n","  else:\n","    predictionsFinale.append([1, ligne[1]])\n","\n","df = pd.DataFrame(predictionsFinale, columns=[\"predictions\", \"probabilités\"])\n","df\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Les prédictions : [0 1 1 0]\n"]}],"source":["\n","print(\"Les prédictions :\", df.predictions.values)\n"]},{"cell_type":"markdown","metadata":{"id":"9EiaxgoI86z8"},"source":["Let's compare the predictions to the true labels, do you notice a \"type\" difference ?"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Les valeurs réelles : [0 1 1 0]\n"]}],"source":["print(\"Les valeurs réelles :\", y_train.flatten())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxtYUSA79AMC"},"outputs":[],"source":["# Please display true values :\n"]},{"cell_type":"markdown","metadata":{"id":"AfJh0hSsg3_d"},"source":["This takes ridicuously long! Let's try to get the training faster. But first of all - and in order to \"measure\" how long the training takes, implement some code which stores the epoch at which the Neural Network has stably reached 100 percent accuracy. Stably means that the accuracy does not jump back to less then 100 percent. There are keras callbacks - and you could write a custom callback. But for now, you can also write a loop - in which the model is trained for one epoch at every iteration. You should store the accuracies at each epoch in a list in order to be able to visualize them.\n","\n","Do not alter the cost function!\n","\n","Hint: Good code style would be to put the model set up into a function and also the code to get the accuracy\n","\n","You should get someting like this format :\n","* Epoch 0 / 200    accuracy: 0.5\n","* Epoch 20 / 200    accuracy: 0.5\n","* Epoch 40 / 200    accuracy: 0.5\n","* Epoch 60 / 200    accuracy: 0.5\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gWSoxT8lPqj"},"outputs":[],"source":["# # function for model-set-up\n","# def build_model():\n","#   # Layers\n","\n","#   # Learning rate\n","\n","#   # Optimizer\n","\n","#   # Compile\n","\n","#   return model\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# function for model-set-up\n","def build_model(X, y, epochs=200):\n","  from keras.models import Sequential\n","  from keras.layers import Dense\n","  from keras.callbacks import LearningRateScheduler\n","  import keras\n","\n","  # Layers\n","  model = Sequential()\n","  model.add(Dense(50, activation='relu', input_dim=2))\n","  model.add(Dense(10, activation='relu'))\n","  model.add(Dense(50, activation=\"relu\"))\n","  model.add(Dense(2, activation=\"softmax\"))\n","\n","  # Création d'une fonction call back qui va être appelé après chaque epoch\n","  # On crée une liste qui récupérera les valeurs d'accuracy\n","  accuracys = []\n","  # La classe callback de keras qui doit hériter de keras.callbacks.Callback\n","  class CustomCallback(keras.callbacks.Callback):\n","    # https://keras.io/guides/writing_your_own_callbacks/\n","    # Le nom on_epoch_end est reconnu par keras, il va lancer la fonction à la fin de chaque epoch\n","    # La fonction récupère l'accuracy\n","    def on_epoch_end(self, accuracy, logs=None):\n","      #keys = list(logs.keys())\n","      # on ajoute l'accuracy récupéré à la fin de chaque tour dans la liste\n","      accuracys.append(logs['accuracy'])\n","      # Si elle est à 100% (1.0)\n","      if logs['accuracy'] == 1.0:\n","        # On arrête l'apprentissage\n","        self.model.stop_training = True\n","      #print(f\"\\nAccuracy ! >>> {logs['accuracy']}\")\n","      # Ensuite on retourne la liste des accuracy\n","      return accuracys\n","\n","  # Learning rate\n","  learning_rate = 0.01\n","\n","  # Optimizer\n","  opt = keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","  # Compile\n","\n","  model.compile(optimizer=opt,\n","              loss='sparse_categorical_crossentropy',\n","              metrics=[\"accuracy\"])\n","\n","  model.fit(X, y, epochs=epochs, callbacks=[CustomCallback()])\n","\n","  return model, accuracys\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","1/1 [==============================] - 1s 1s/step - loss: 0.6899 - accuracy: 0.7500\n","Epoch 2/200\n","1/1 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.5000\n","Epoch 3/200\n","1/1 [==============================] - 0s 5ms/step - loss: 0.6694 - accuracy: 0.5000\n","Epoch 4/200\n","1/1 [==============================] - 0s 5ms/step - loss: 0.6527 - accuracy: 0.5000\n","Epoch 5/200\n","1/1 [==============================] - 0s 6ms/step - loss: 0.6364 - accuracy: 0.7500\n","Epoch 6/200\n","1/1 [==============================] - 0s 5ms/step - loss: 0.6125 - accuracy: 1.0000\n"]}],"source":["model, accuracy = build_model(x_train, y_train.flatten(), epochs=200)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n","20/20 [==============================] - 1s 1ms/step - loss: 0.6509 - accuracy: 0.6000 \n","Epoch 2/60\n","20/20 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 1.0000\n","Epoch 3/60\n","20/20 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 1.0000\n","Epoch 4/60\n","20/20 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 1.0000\n","Epoch 5/60\n","20/20 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 1.0000\n","Epoch 6/60\n","20/20 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 7/60\n","20/20 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 8/60\n","20/20 [==============================] - 0s 1ms/step - loss: 6.4797e-04 - accuracy: 1.0000\n","Epoch 9/60\n","20/20 [==============================] - 0s 1ms/step - loss: 4.1728e-04 - accuracy: 1.0000\n","Epoch 10/60\n","20/20 [==============================] - 0s 1ms/step - loss: 2.8469e-04 - accuracy: 1.0000\n","Epoch 11/60\n","20/20 [==============================] - 0s 1ms/step - loss: 2.1139e-04 - accuracy: 1.0000\n","Epoch 12/60\n","20/20 [==============================] - 0s 1ms/step - loss: 1.6500e-04 - accuracy: 1.0000\n","Epoch 13/60\n","WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1200 batches). You may need to use the repeat() function when building your dataset.\n","20/20 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"]},{"data":{"text/plain":["[0.6000000238418579,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 0.0]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import LearningRateScheduler\n","import keras\n","#import tensorflow_addons as tfa\n","\n","\n","# Layers\n","model = Sequential()\n","model.add(Dense(50, activation='relu', input_dim=2))\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(50, activation=\"relu\"))\n","model.add(Dense(2, activation=\"softmax\"))\n","\n","# Learning rate\n","learning_rate = 0.01\n","\n","# Optimizer\n","opt = keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","# Compile\n","\n","model.compile(optimizer=opt,\n","            loss='sparse_categorical_crossentropy',\n","            metrics=[\"accuracy\"])\n","\n","X = x_train\n","y = y_train.flatten()\n","\n","def fit_model_and_get_accuracy(model, epochs=60, epoch_log_step=20):\n","  model.fit(X, y, epochs=epochs, steps_per_epoch=epoch_log_step)\n","  accuracies_list = model.history.history[\"accuracy\"]\n","  return accuracies_list\n","\n","fit_model_and_get_accuracy(model)\n"]},{"cell_type":"markdown","metadata":{"id":"q0NVM5iWLB5A"},"source":["## First Simple Reference Model (hidden_shape = 2, hidden_size = 1)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ZCdauNB7LEPA"},"outputs":[],"source":["# Use your functions to execute your script\n","\n","# Use your functions to execute your script\n","\n","# function for model-set-up\n","def build_model(X, y, epochs=200):\n","  from keras.models import Sequential\n","  from keras.layers import Dense\n","  from keras.callbacks import LearningRateScheduler\n","  import keras\n","\n","  # Layers\n","  model = Sequential()\n","  model.add(Dense(50, activation='relu', input_dim=2))\n","  model.add(Dense(10, activation='relu'))\n","  model.add(Dense(21, activation=\"relu\"))\n","  model.add(Dense(2, activation=\"softmax\"))\n","\n","  # Fonction call back\n","  accuracys = []\n","  class CustomCallback(keras.callbacks.Callback):\n","    # https://keras.io/guides/writing_your_own_callbacks/\n","    def on_epoch_end(self, accuracy, logs=None):\n","      #keys = list(logs.keys())\n","      accuracys.append(logs['accuracy'])\n","      if logs['accuracy'] == 1.0:\n","        self.model.stop_training = True\n","      #print(f\"\\nAccuracy ! >>> {logs['accuracy']}\")\n","      return accuracys\n","\n","  # Learning rate\n","  learning_rate = 0.01\n","\n","  # Optimizer\n","  opt = keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","  # Compile\n","\n","  model.compile(optimizer=opt,\n","              loss='sparse_categorical_crossentropy',\n","              metrics=[\"accuracy\"])\n","\n","  model.fit(X, y, epochs=epochs)\n","\n","  return model, accuracys\n"]},{"cell_type":"markdown","metadata":{"id":"4DF_vJloNGHU"},"source":["## More complex models"]},{"cell_type":"markdown","metadata":{"id":"A6DkKkEivvr7"},"source":["Okay, now let's get startet. Make a note of your accuracy == 1.0 epoch. It is your baseline. And then try to alter the model so that it trains faster. These are the hyperparameters you need to optimize - but feel free to add others!\n","\n","1) Hidden size\n","\n","2) Number of hidden layers\n","\n","3) Learning rate - just try different numbers\n","   What happens if the learning rate is too big?\n","\n","4) Learning rate - try to decrease it during the training process\n","\n","5) Different optimizers\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B5GksrFrgjOz"},"source":["For the eager ones: The weights are initialized randomly (within limits) - so if you want better results and insights into the effect of different hyperparameters, you would have to run each experiment a couple of times (i.e. 5 to 10 minimum) and average over them. But you may ignore this in this quest - just bear it in mind!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJHDHgzBn8an"},"outputs":[],"source":["# Hidden Size\n","\n","\n","# Number of hidden layers\n","\n","# Learning rate\n","\n","# Learning rate - continuous decrease\n","\n","# Optimizers\n","model, accuracy = build_model(x_train, y_train.flatten(), epochs=200)\n","accuracy\n"]},{"cell_type":"markdown","metadata":{"id":"pk5KMJH50oEj"},"source":["Now visualize all your learning curves - i.e. nr. of epochs against accuracy. Which hyperparameter did have the biggest effect?\n","\n","Hint: In order to do that systematically, you could save the list of accuracies for each experiment and then display them all in one graph (at least for each tuned hyperparameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmMcqtDI2Yd8"},"outputs":[],"source":["# Here go the plots\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(model.history.history['accuracy'])\n","#plt.plot(model.history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","#plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bApQTK9s2bt7"},"outputs":[],"source":["# More plots\n","import matplotlib.pyplot as plt\n","\n","\n","plt.plot(model.history.history['loss'])\n","#plt.plot(model.history.history['val_accuracy'])\n","plt.title('model MSE')\n","plt.ylabel('MSE')\n","plt.xlabel('epoch')\n","#plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i79yRkf92dhB"},"outputs":[],"source":["# Add as many plots as you want\n"]},{"cell_type":"markdown","metadata":{"id":"26JvNPnp4GX-"},"source":["We are still working with mean_squared_error as a cost-function. What would be a more suitable cost function?\n","\n","Alter the cost function and see how fast you get to a stable accuracy of 1.0.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvF_h8ov4KKl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yc1RlJX02gpW"},"source":["What is your best score (epoch with accuracy == 1.0)?\n","\n","Can you get the score to under 100?\n","\n","What might be the problem with your best model?\n","\n","Answer all these questions in the text cell below"]},{"cell_type":"markdown","metadata":{"id":"GHCxtSad32fD"},"source":["My conclusion about these experiments is:"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[{"file_id":"1IsLW3ATseVaOkzdDq8FCKtkvyu4cCtHx","timestamp":1706174051170},{"file_id":"1RcNhp6MwYDR_5BIbXI3u0u1YD-YkWVNE","timestamp":1614841480751}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
