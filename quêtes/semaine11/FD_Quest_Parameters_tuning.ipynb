{"cells":[{"cell_type":"markdown","metadata":{"id":"tLTWTTh057Bz"},"source":["### The XOR-Model ###\n","\n","We will build our tiny neural network predicting the XOR-data in keras\n","\n","First of all, we need to import the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"p9mkaLawe9zf"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"]},{"cell_type":"markdown","metadata":{"id":"7eYcdsPse4f2"},"source":["We need to define simple datas.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo7zQOL-6sOL"},"outputs":[],"source":["#Input datasets\n","x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y_train = np.array([[0],[1],[1],[0]])\n"]},{"cell_type":"markdown","metadata":{"id":"_QPZyiW56zoW"},"source":["Now we set up our parameters - and define the model, exactly as in the lecture slides\n","\n","TODO: Show different way to set up the model, with and without Input layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwYszp9qoxzh"},"outputs":[],"source":["# Set up your model here :\n"]},{"cell_type":"markdown","metadata":{"id":"fMBXTEQg7Ejx"},"source":["We already have compiled our model - now we need to train it. We also need to get some predictions in order to see whether our model can indeed predict the XOR-data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbDOj8JW7JTV"},"outputs":[],"source":["# Train your model here, and predict the XOR-data :\n"]},{"cell_type":"markdown","metadata":{"id":"Ojlc0RHB6vXA"},"source":["What is wrong with these predictions?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOlTwqPMgOQk"},"outputs":[],"source":["# Please display predicted values :\n"]},{"cell_type":"markdown","metadata":{"id":"9EiaxgoI86z8"},"source":["Let's compare the predictions to the true labels, do you notice a \"type\" difference ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxtYUSA79AMC"},"outputs":[],"source":["# Please display true values :\n"]},{"cell_type":"markdown","metadata":{"id":"AfJh0hSsg3_d"},"source":["This takes ridicuously long! Let's try to get the training faster. But first of all - and in order to \"measure\" how long the training takes, implement some code which stores the epoch at which the Neural Network has stably reached 100 percent accuracy. Stably means that the accuracy does not jump back to less then 100 percent. There are keras callbacks - and you could write a custom callback. But for now, you can also write a loop - in which the model is trained for one epoch at every iteration. You should store the accuracies at each epoch in a list in order to be able to visualize them.\n","\n","Do not alter the cost function!\n","\n","Hint: Good code style would be to put the model set up into a function and also the code to get the accuracy\n","\n","You should get someting like this format :\n","* Epoch 0 / 200    accuracy: 0.5\n","* Epoch 20 / 200    accuracy: 0.5\n","* Epoch 40 / 200    accuracy: 0.5\n","* Epoch 60 / 200    accuracy: 0.5\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gWSoxT8lPqj"},"outputs":[],"source":["# function for model-set-up\n","def build_model():\n","  # Layers\n","\n","  # Learning rate\n","\n","  # Optimizer\n","\n","  # Compile\n","\n","  return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_ETWw9SKqZ2"},"outputs":[],"source":["def fit_model_and_get_accuracy(model, epochs=60, epoch_log_step=20):\n","\n","\n","  return accuracies_list\n"]},{"cell_type":"markdown","metadata":{"id":"q0NVM5iWLB5A"},"source":["## First Simple Reference Model (hidden_shape = 2, hidden_size = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCdauNB7LEPA"},"outputs":[],"source":["# Use your functions to execute your script\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4DF_vJloNGHU"},"source":["## More complex models"]},{"cell_type":"markdown","metadata":{"id":"A6DkKkEivvr7"},"source":["Okay, now let's get startet. Make a note of your accuracy == 1.0 epoch. It is your baseline. And then try to alter the model so that it trains faster. These are the hyperparameters you need to optimize - but feel free to add others!\n","\n","1) Hidden size\n","\n","2) Number of hidden layers\n","\n","3) Learning rate - just try different numbers\n","   What happens if the learning rate is too big?\n","\n","4) Learning rate - try to decrease it during the training process\n","\n","5) Different optimizers\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B5GksrFrgjOz"},"source":["For the eager ones: The weights are initialized randomly (within limits) - so if you want better results and insights into the effect of different hyperparameters, you would have to run each experiment a couple of times (i.e. 5 to 10 minimum) and average over them. But you may ignore this in this quest - just bear it in mind!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJHDHgzBn8an"},"outputs":[],"source":["# Hidden Size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pecBN-FN0WyS"},"outputs":[],"source":["# Number of hidden layers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDLRcqsh0Z09"},"outputs":[],"source":["# Learning rate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69fc0vwA0cvJ"},"outputs":[],"source":["# Learning rate - continuous decrease\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbq6fu-G0ii8"},"outputs":[],"source":["# Optimizers\n"]},{"cell_type":"markdown","metadata":{"id":"pk5KMJH50oEj"},"source":["Now visualize all your learning curves - i.e. nr. of epochs against accuracy. Which hyperparameter did have the biggest effect?\n","\n","Hint: In order to do that systematically, you could save the list of accuracies for each experiment and then display them all in one graph (at least for each tuned hyperparameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmMcqtDI2Yd8"},"outputs":[],"source":["# Here go the plots\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bApQTK9s2bt7"},"outputs":[],"source":["# More plots\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i79yRkf92dhB"},"outputs":[],"source":["# Add as many plots as you want\n"]},{"cell_type":"markdown","metadata":{"id":"26JvNPnp4GX-"},"source":["We are still working with mean_squared_error as a cost-function. What would be a more suitable cost function?\n","\n","Alter the cost function and see how fast you get to a stable accuracy of 1.0.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvF_h8ov4KKl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"yc1RlJX02gpW"},"source":["What is your best score (epoch with accuracy == 1.0)?\n","\n","Can you get the score to under 100?\n","\n","What might be the problem with your best model?\n","\n","Answer all these questions in the text cell below"]},{"cell_type":"markdown","metadata":{"id":"GHCxtSad32fD"},"source":["My conclusion about these experiments is:"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[{"file_id":"1IsLW3ATseVaOkzdDq8FCKtkvyu4cCtHx","timestamp":1706174051170},{"file_id":"1RcNhp6MwYDR_5BIbXI3u0u1YD-YkWVNE","timestamp":1614841480751}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
