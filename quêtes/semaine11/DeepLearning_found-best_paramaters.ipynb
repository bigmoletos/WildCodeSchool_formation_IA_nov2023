{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche d'une fonction d'optimisation d'une architecture d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des differents types de DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les 5 grands types de datasets couramment rencontrés dans le domaine de la science des données :\n",
    "\n",
    "***Données Structurées*** :\n",
    "\n",
    "- Ce sont des données qui ont une organisation fixe, généralement sous forme de tableaux avec des lignes et des colonnes. \n",
    "- Chaque colonne représente une variable et chaque ligne une observation.\n",
    "Exemples : bases de données SQL, feuilles de calcul Excel, CSV.\n",
    "\n",
    "***Données Non Structurées*** :\n",
    "\n",
    "- Contrairement aux données structurées, les données non structurées ne suivent pas un modèle ou un format spécifique. \n",
    "\n",
    "- Elles sont plus complexes à analyser et à traiter.\n",
    "\n",
    "Exemples : texte libre (emails, articles), images, vidéos, enregistrements audio.\n",
    "\n",
    "***Données Semi-Structurées*** :\n",
    "\n",
    "- Ces données ne sont pas organisées dans des tableaux rigides comme les données structurées, mais contiennent néanmoins des marqueurs ou des étiquettes qui séparent différents éléments.\n",
    "Exemples : documents JSON, XML.\n",
    "\n",
    "***Données Temporelles (Time Series)*** :\n",
    "\n",
    "- Il s'agit de données qui sont collectées, enregistrées ou organisées en fonction du temps. \n",
    "- Elles sont utilisées pour analyser les tendances, les prévisions, etc.\n",
    "Exemples : enregistrements boursiers, données météorologiques historiques, séries chronologiques de ventes.\n",
    "\n",
    "***Données Spatiales (ou Géospatiales)*** :\n",
    "\n",
    "- Ce type de données est lié à des informations géographiques ou spatiales. \n",
    "- Elles sont souvent utilisées pour des analyses cartographiques ou de localisation.\n",
    "Exemples : cartes, données GPS, données de localisation de téléphones mobiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing top 5 par type de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***Données Structurées***\n",
    "Nettoyage des Données :\n",
    "\n",
    "- Gestion des valeurs manquantes : imputation, suppression des lignes/colonnes.\n",
    "- Correction des erreurs et incohérences : valeurs aberrantes, erreurs de saisie.\n",
    "- Transformation des Données :\n",
    "\n",
    "- Normalisation/Standardisation : mise à l'échelle des variables pour une plage commune.\n",
    "- Encodage : conversion des variables catégorielles en numériques (One-Hot Encoding, Label Encoding).\n",
    "- Réduction de Dimensionnalité : Analyse en Composantes Principales (PCA), Sélection de Caractéristiques.\n",
    "\n",
    "2. ***Données Non Structurées***\n",
    "Texte :\n",
    "\n",
    "- Nettoyage : suppression des balises HTML, des caractères spéciaux, et mise en minuscules.\n",
    "- Tokenisation : découpage en mots ou phrases.\n",
    "Suppression des mots vides (stop words), Stemming/Lemmatisation.\n",
    "- Encodage : Bag of Words, TF-IDF, Word Embeddings.\n",
    "\n",
    "*Images et Vidéos* :\n",
    "\n",
    "- Normalisation : ajustement de la taille, mise à l'échelle des valeurs de pixels.\n",
    "- Augmentation des données : rotation, translation, retournement pour augmenter la taille du dataset.\n",
    "- Extraction de caractéristiques : utilisation de réseaux de neurones convolutifs (CNN).\n",
    "\n",
    "*Audio* :\n",
    "\n",
    "Extraction de caractéristiques : spectrogrammes, MFCC (Mel-Frequency Cepstral Coefficients).\n",
    "Réduction du bruit.\n",
    "\n",
    "3. ***Données Semi-Structurées***\n",
    "- Extraction de Caractéristiques :\n",
    "Conversion de formats JSON ou XML en structures tabulaires.\n",
    "Extraction de champs spécifiques et transformation en données structurées pour une analyse plus poussée.\n",
    "\n",
    "4. ***Données Temporelles (Time Series)***\n",
    "Transformation :\n",
    "\n",
    "Découpage en fenêtres temporelles.\n",
    "Création de caractéristiques basées sur le temps : tendances, saisonnalité.\n",
    "- Normalisation :\n",
    "\n",
    "- Standardisation des séries temporelles.\n",
    "- Décomposition :\n",
    "\n",
    "Séparation des composantes tendancielles, saisonnières et résiduelles.\n",
    "\n",
    "5. ***Données Spatiales (ou Géospatiales)***\n",
    "Transformation des Coordonnées :\n",
    "\n",
    "- Conversion des formats de coordonnées, ajustement aux systèmes de référence.\n",
    "- Traitement des Données Raster et Vectorielles :\n",
    "\n",
    "- Nettoyage, simplification des formes, superposition de couches d'informations.\n",
    "- Extraction de Caractéristiques :\n",
    "\n",
    "Création de caractéristiques basées sur la localisation (par exemple, distance à un point d'intérêt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'évaluation du type de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def determine_dataset_type(dataset):\n",
    "    # Vérifie si le dataset est un DataFrame pandas (données structurées)\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # Vérifie la présence de colonnes temporelles\n",
    "        if any(pd.api.types.is_datetime64_any_dtype(dataset[col]) for col in dataset.columns):\n",
    "            return \"Time Series (Temporal Data)\"\n",
    "        # Vérifie la présence de colonnes spatiales (latitude/longitude)\n",
    "        if 'latitude' in dataset.columns and 'longitude' in dataset.columns:\n",
    "            return \"Spatial Data\"\n",
    "        return \"Structured Data\"\n",
    "\n",
    "    # Vérifie si le dataset est une série de textes ou de fichiers (données non structurées)\n",
    "    if isinstance(dataset, (list, pd.Series)) and all(isinstance(x, str) for x in dataset):\n",
    "        return \"Unstructured Data (Text or Files)\"\n",
    "\n",
    "    # Vérifie si le dataset est sous forme de dictionnaires ou de JSON (semi-structuré)\n",
    "    if isinstance(dataset, (dict, list)) and all(isinstance(x, (dict, list)) for x in dataset):\n",
    "        return \"Semi-Structured Data (e.g., JSON)\"\n",
    "\n",
    "    return \"Unknown Type\"\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Remplacez par le chemin de votre fichier de données\n",
    "dataset = pd.read_csv('SpotifyFeatures.csv')\n",
    "\n",
    "print(determine_dataset_type(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iris Dataset\n",
    "Description : Contient des mesures de longueur et largeur de sépales et pétales pour trois espèces d'iris.\n",
    "Type Attendu : Structured Data\n",
    "2. Enron Email Dataset\n",
    "Description : Collection d'environ 500 000 emails provenant de cadres de l'entreprise Enron.\n",
    "Type Attendu : Unstructured Data (Text or Files)\n",
    "3. New York City Taxi Trip Records\n",
    "Description : Contient des enregistrements détaillés des trajets en taxi à New York, y compris les coordonnées de départ et d'arrivée.\n",
    "Type Attendu : Spatial Data\n",
    "4. Bitcoin Historical Data\n",
    "Description : Données historiques sur le prix du Bitcoin, y compris la date et l'heure des transactions.\n",
    "Type Attendu : Time Series (Temporal Data)\n",
    "5. OpenStreetMap Data\n",
    "Description : Contient des données géospatiales sous forme de fichiers XML, y compris des informations sur les routes, les bâtiments, etc.\n",
    "Type Attendu : Semi-Structured Data (e.g., JSON) - Bien que les données OpenStreetMap soient souvent en XML, la fonction pourrait les identifier comme semi-structurées en raison de leur nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iris Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print(determine_dataset_type(iris_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Enron Email Dataset (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Type\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de mails sous forme de liste de chaînes de caractères\n",
    "# Ajoutez des exemples de mails\n",
    "emails = [\"Here is the meeting agenda...\",\n",
    "          \"Please find attached the report...\", ...]\n",
    "print(determine_dataset_type(emails))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. New York City Taxi Trip Records (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Data\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de trajets de taxi avec des coordonnées\n",
    "taxi_data = pd.DataFrame({\n",
    "    'pickup_latitude': [40.761432, 40.644102, ...],  # Ajoutez des coordonnées\n",
    "    'pickup_longitude': [-73.979815, -73.781632, ...],\n",
    "    'dropoff_latitude': [40.641233, 40.729523, ...],\n",
    "    'dropoff_longitude': [-73.958763, -73.991567, ...]\n",
    "})\n",
    "print(determine_dataset_type(taxi_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bitcoin Historical Data (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series (Temporal Data)\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de données historiques Bitcoin\n",
    "btc_data = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2021-01-01', periods=100, freq='D'),\n",
    "    'price': np.random.uniform(30000, 60000, 100)  # Prix aléatoires\n",
    "})\n",
    "print(determine_dataset_type(btc_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. OpenStreetMap Data (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Type\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de données géospatiales (XML/JSON-like structure)\n",
    "osm_data = [\n",
    "    {\"type\": \"node\", \"id\": 1, \"lat\": 59.941, \"lon\": 30.313},\n",
    "    {\"type\": \"node\", \"id\": 2, \"lat\": 59.942, \"lon\": 30.314},\n",
    "    ...  # Ajoutez des données fictives similaires\n",
    "]\n",
    "print(determine_dataset_type(osm_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing des datasets\n",
    "### Preprocessing pour les Données structurées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def preprocess_structured_data(dataset):\n",
    "    # Prétraitement pour les données structurées\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    dataset = pd.DataFrame(imputer.fit_transform(\n",
    "        dataset), columns=dataset.columns)\n",
    "    scaler = StandardScaler()\n",
    "    dataset = pd.DataFrame(scaler.fit_transform(\n",
    "        dataset), columns=dataset.columns)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données non structurées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def preprocess_unstructured_data(dataset):\n",
    "    # Prétraitement pour les données textuelles non structurées\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    dataset = vectorizer.fit_transform(dataset)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données de type times series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_temporal_data(dataset):\n",
    "    # Prétraitement pour les données temporelles\n",
    "    dataset.fillna(method='ffill', inplace=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données Spatiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def preprocess_spatial_data(dataset):\n",
    "    # Supposons que dataset est un DataFrame avec des colonnes 'latitude' et 'longitude'\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset[['latitude', 'longitude']] = scaler.fit_transform(\n",
    "        dataset[['latitude', 'longitude']])\n",
    "\n",
    "    # Calcul d'une caractéristique dérivée, par exemple, la distance depuis le centre-ville\n",
    "    city_center = (48.8566, 2.3522)  # Coordonnées de Paris, par exemple\n",
    "    dataset['distance_from_center'] = np.sqrt(\n",
    "        (dataset['latitude'] - city_center[0])**2 + (dataset['longitude'] - city_center[1])**2)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "spatial_data = pd.DataFrame({\n",
    "    'latitude': np.random.uniform(48.8, 48.9, 100),\n",
    "    'longitude': np.random.uniform(2.3, 2.4, 100)\n",
    "})\n",
    "preprocessed_spatial_data = preprocess_spatial_data(spatial_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing pour les Données Semi-Structurées (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def preprocess_json_data(json_data):\n",
    "    # Convertir le JSON en DataFrame\n",
    "    dataset = pd.json_normalize(json_data)\n",
    "\n",
    "    # Gérer les valeurs manquantes\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    dataset = pd.DataFrame(imputer.fit_transform(\n",
    "        dataset), columns=dataset.columns)\n",
    "\n",
    "    # Encoder les variables catégorielles\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_columns = encoder.fit_transform(dataset[['interests']]).toarray()\n",
    "    dataset = dataset.join(pd.DataFrame(\n",
    "        encoded_columns, columns=encoder.get_feature_names_out(['interests'])))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "json_data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"interests\": \"music\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"interests\": \"sports\"},\n",
    "    # ... autres données\n",
    "]\n",
    "preprocessed_json_data = preprocess_json_data(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction globale de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset_type = determine_dataset_type(dataset)\n",
    "\n",
    "    if dataset_type == \"Structured Data\":\n",
    "        return preprocess_structured_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Unstructured Data (Text or Files)\":\n",
    "        return preprocess_unstructured_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Time Series (Temporal Data)\":\n",
    "        return preprocess_temporal_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Spatial Data\":\n",
    "        return preprocess_spatial_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Semi-Structured Data (e.g., JSON)\":\n",
    "        return preprocess_json_data(dataset)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Type de dataset non reconnu ou non pris en charge pour le prétraitement\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    # Identifier le type de dataset\n",
    "    dataset_type = determine_dataset_type(dataset)\n",
    "\n",
    "    if dataset_type == \"Structured Data\":\n",
    "        # Prétraitement pour les données structurées\n",
    "        # Gérer les valeurs manquantes\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        dataset = pd.DataFrame(imputer.fit_transform(\n",
    "            dataset), columns=dataset.columns)\n",
    "\n",
    "        # Encoder les variables catégorielles\n",
    "        dataset = pd.get_dummies(dataset)\n",
    "\n",
    "        # Normaliser les données\n",
    "        scaler = StandardScaler()\n",
    "        dataset = pd.DataFrame(scaler.fit_transform(\n",
    "            dataset), columns=dataset.columns)\n",
    "\n",
    "    elif dataset_type == \"Unstructured Data (Text or Files)\":\n",
    "        # Prétraitement pour les données textuelles non structurées\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        dataset = vectorizer.fit_transform(dataset)\n",
    "\n",
    "    elif dataset_type == \"Time Series (Temporal Data)\":\n",
    "        # Prétraitement pour les données temporelles\n",
    "        # Remplir les valeurs manquantes\n",
    "        dataset.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    elif dataset_type == \"Spatial Data\":\n",
    "        # Prétraitement pour les données spatiales\n",
    "        # Supposer que les données sont déjà sous forme numérique appropriée pour l'analyse\n",
    "        pass\n",
    "\n",
    "    elif dataset_type == \"Semi-Structured Data (e.g., JSON)\":\n",
    "        # Prétraitement pour les données semi-structurées\n",
    "        # Convertir en DataFrame (si nécessaire) et appliquer le prétraitement comme pour les données structurées\n",
    "        # Cette étape dépendra fortement de la structure spécifique des données semi-structurées\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Type de dataset non reconnu ou non pris en charge pour le prétraitement\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Utiliser la fonction sur un dataset spécifique\n",
    "# dataset = ...  # Charger ou définir le dataset\n",
    "# preprocessed_dataset = preprocess_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place de l'API kaggle pour receuillir l'architecture des 300 meilleurs competiteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kaggle in c:\\users\\romar\\appdata\\roaming\\python\\python311\\site-packages (1.6.3)\n",
      "Requirement already satisfied: six>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.1)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/97/e3/fd7272d6d2c49fd49a79a603cb28c8b5a71f8911861b4a0409b3c006a241/selenium-4.17.2-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.17.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/14/fb/9299cf74953f473a15accfdbe2c15218e766bae8c796f2567c83bae03e98/trio-0.24.0-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.24.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium)\n",
      "  Obtaining dependency information for typing_extensions>=4.9.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.9 MB 1.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/9.9 MB 3.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/9.9 MB 5.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 7.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.8/9.9 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.9/9.9 MB 13.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.9 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.8/9.9 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 18.1 MB/s eta 0:00:00\n",
      "Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "   ---------------------------------------- 0.0/460.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 460.2/460.2 kB 28.1 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing_extensions, sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.17.2 sniffio-1.3.0 trio-0.24.0 trio-websocket-0.11.1 typing_extensions-4.9.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to locate or obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Votre code ici pour travailler avec le navigateur\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialisation du service Chrome avec le chemin vers le chromedriver\u001b[39;00m\n\u001b[0;32m     18\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(executable_path\u001b[38;5;241m=\u001b[39mpath_to_webdriver)\n\u001b[1;32m---> 19\u001b[0m browser \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Accès à la page de classement Kaggle\u001b[39;00m\n\u001b[0;32m     22\u001b[0m browser\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/rankings?group=competitions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     46\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mDesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m     vendor_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m     49\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[0;32m     50\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:49\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new WebDriver instance of the ChromiumDriver. Starts the\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mservice and then creates new WebDriver instance of ChromiumDriver.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m - keep_alive - Whether to configure ChromiumRemoteConnection to use HTTP keep-alive.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m DriverFinder\u001b[38;5;241m.\u001b[39mget_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:44\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate or obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to locate or obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "\n",
    "# Initialisation du navigateur sans spécifier le chemin d'accès du driver\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Votre code ici pour travailler avec le navigateur\n",
    "\n",
    "\n",
    "# Chemin vers le driver de votre navigateur (par exemple, chromedriver pour Google Chrome)\n",
    "# path_to_webdriver = 'chemin/vers/votre/chromedriver'\n",
    "\n",
    "# Initialisation du service Chrome avec le chemin vers le chromedriver\n",
    "service = Service(executable_path=path_to_webdriver)\n",
    "browser = webdriver.Chrome(service=service)\n",
    "\n",
    "# Accès à la page de classement Kaggle\n",
    "browser.get('https://www.kaggle.com/rankings?group=competitions')\n",
    "\n",
    "# Attendre que le contenu soit chargé\n",
    "try:\n",
    "    # Modifier la valeur de By.CLASS_NAME pour correspondre à la classe qui contient les utilisateurs\n",
    "    element = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CLASS_NAME, 'nom_de_classe_pour_les_utilisateurs'))\n",
    "    )\n",
    "    # Récupérer les informations des utilisateurs après le chargement du contenu\n",
    "    user_elements = browser.find_elements(By.CLASS_NAME, 'nom_de_classe_pour_les_utilisateurs')\n",
    "    for user_element in user_elements:\n",
    "        # Extraire les informations ici\n",
    "        pass\n",
    "finally:\n",
    "    browser.quit()\n",
    "\n",
    "# Continuer le traitement des données récupérées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la requête GET : 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la page de la compétition Kaggle\n",
    "competition_url = \"https://www.kaggle.com/c/how-to-preprocessing-when-using-embeddings/participants\"  # Remplacez par l'URL de la compétition\n",
    "\n",
    "# Envoyer une requête GET pour récupérer la page HTML\n",
    "response = requests.get(competition_url)\n",
    "\n",
    "# Vérifier si la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Analyser la page HTML avec BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Trouver la section contenant la liste des compétiteurs\n",
    "    competitors_section = soup.find(\"div\", class_=\"ParticipantsHeader_title_1b6Gh\")\n",
    "\n",
    "    if competitors_section:\n",
    "        # Extraire la liste des compétiteurs\n",
    "        competitors_list = competitors_section.find_next(\"ul\")\n",
    "\n",
    "        # Afficher la liste des compétiteurs\n",
    "        for competitor in competitors_list.find_all(\"li\"):\n",
    "            competitor_name = competitor.text.strip()\n",
    "            print(f\"Nom du compétiteur : {competitor_name}\")\n",
    "    else:\n",
    "        print(\"La section des compétiteurs n'a pas été trouvée sur la page.\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête GET : {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "competition :\n",
      "https://www.kaggle.com/competitions/blood-vessel-segmentation \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KaggleApi' object has no attribute 'competition_list_teams_with_http_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcompetition :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcompetition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Obtenez la liste des compétiteurs de la compétition\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m competitors, _, _ \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mcompetition_list_teams_with_http_info(competition)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Affichez la liste des compétiteurs\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m competitor \u001b[38;5;129;01min\u001b[39;00m competitors:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KaggleApi' object has no attribute 'competition_list_teams_with_http_info'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from tqdm import tqdm  # Importez tqdm\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# # Obtenez la liste des compétiteurs de la compétition\n",
    "# competitors = api.competition_list_teams(competition_name)\n",
    "\n",
    "# for competitor in competitors:\n",
    "#     print(competitor.userName, competitor.performanceTier)\n",
    "\n",
    "\n",
    "\n",
    "# Lister les competitions disponibles\n",
    "competitions = api.competitions_list()\n",
    "for competition in tqdm(competitions):\n",
    "    print(f\"\\ncompetition :\\n{competition} \\n\")\n",
    "    # Obtenez la liste des compétiteurs de la compétition\n",
    "    competitors, _, _ = api.competition_list_teams_with_http_info(competition)\n",
    "    # Affichez la liste des compétiteurs\n",
    "    for competitor in competitors:\n",
    "        print(f\"Nom du compétiteur : {competitor['name']}\")\n",
    "\n",
    "# Lister les datasets disponibles\n",
    "datasets = api.datasets_list()\n",
    "# print(f\"\\n datasets:\\n{datasets[0:10]} \\n\")\n",
    "# info des 10 premiers\n",
    "for dataset in tqdm(datasets[:10]):\n",
    "    title = dataset.get('title', 'Titre non disponible')\n",
    "    url = dataset.get('url', 'URL non disponible')\n",
    "    print(f\" Dataset Titre: {title}, URL: {url}\")\n",
    "\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "    title = dataset['titleNullable'] if 'titleNullable' in dataset else 'Titre non disponible'\n",
    "    url = dataset['urlNullable'] if 'urlNullable' in dataset else 'URL non disponible'\n",
    "    print(f\" DAtaset Titre: {title}, URL: {url}\")\n",
    "\n",
    "kernels = api.kernels_list()\n",
    "# print(f\"\\n kernels:\\n{kernels[:10]} \\n\")\n",
    "for kernel in kernel_objects:\n",
    "    title = kernel.title\n",
    "    author = kernel.author.get('userName', 'Auteur non disponible')\n",
    "    url = kernel.url\n",
    "    print(f\"Titre: {title}, Auteur: {author}, URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "api :\n",
      "<kaggle.api.kaggle_api_extended.KaggleApi object at 0x00000177BA716450>,api.authenticate(): None \n",
      "\n",
      "Titre: Jobs and Salaries in Data Science, URL: https://www.kaggle.com/datasets/hummaamqaasim/jobs-in-data\n",
      "Titre: Apple Quality, URL: https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality\n",
      "Titre: 57,650 Spotify Songs, URL: https://www.kaggle.com/datasets/joebeachcapital/57651-spotify-songs\n",
      "Titre: Gold Stock Prices, URL: https://www.kaggle.com/datasets/sahilwagh/gold-stock-prices\n",
      "Titre: Heart Attack Prediction, URL: https://www.kaggle.com/datasets/juledz/heart-attack-prediction\n",
      "Titre: TATA Motors Limited - Stock Prices (2006-2024), URL: https://www.kaggle.com/datasets/raunakpandey030/tata-motors-limited-stock-prices-2006-2024\n",
      "Titre: New York Housing Market, URL: https://www.kaggle.com/datasets/nelgiriyewithana/new-york-housing-market\n",
      "Titre: IMDB Movie Dataset Till Dec-2023, URL: https://www.kaggle.com/datasets/kianindeed/imdb-movie-dataset-dec-2023\n",
      "Titre: NFL Team Data 2003-2023, URL: https://www.kaggle.com/datasets/nickcantalupa/nfl-team-data-2003-2023\n",
      "Titre: Sales Store overview, URL: https://www.kaggle.com/datasets/fekihmea/sales-store-overview\n",
      "Titre: DAIGT-V4-TRAIN-DATASET, URL: https://www.kaggle.com/datasets/thedrcat/daigt-v4-train-dataset\n",
      "Titre: Car Sales Report, URL: https://www.kaggle.com/datasets/missionjee/car-sales-report\n",
      "Titre: Brain-Spectrograms, URL: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n",
      "Titre: International Student Demographics, URL: https://www.kaggle.com/datasets/webdevbadger/international-student-demographics\n",
      "Titre: Student Exam Performance Prediction, URL: https://www.kaggle.com/datasets/mrsimple07/student-exam-performance-prediction\n",
      "Titre: Energy-consumption-prediction, URL: https://www.kaggle.com/datasets/mrsimple07/energy-consumption-prediction\n",
      "Titre: World data population, URL: https://www.kaggle.com/datasets/tanishqdublish/world-data-population\n",
      "Titre: Glassdoor Data science Jobs - 2024, URL: https://www.kaggle.com/datasets/kuralamuthan300/glassdoor-data-science-jobs\n",
      "Titre: Depression and Mental Health Data Analysis, URL: https://www.kaggle.com/datasets/shashwatwork/depression-and-mental-health-data-analysis\n",
      "Titre: Data Science Salaries 2024, URL: https://www.kaggle.com/datasets/sazidthe1/data-science-salaries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# print(f\"\\napi :\\n{api},api.authenticate(): {api.authenticate()} \\n\")\n",
    "\n",
    "# Exemple pour récupérer une liste d'utilisateurs\n",
    "# Note : Cette méthode ne garantit pas d'obtenir les 300 meilleurs compétiteurs\n",
    "# users = api.users_list(page=1, max_results=300)\n",
    "# for user in users:\n",
    "#     print(user.userName, user.performanceTier)\n",
    "# Lister les datasets disponibles\n",
    "# Lister les datasets disponibles\n",
    "datasets = api.datasets_list()\n",
    "# print(f\"\\n datasets:\\n{datasets} \\n\")\n",
    "for dataset in datasets:\n",
    "    title = dataset['titleNullable'] if 'titleNullable' in dataset else 'Titre non disponible'\n",
    "    url = dataset['urlNullable'] if 'urlNullable' in dataset else 'URL non disponible'\n",
    "    print(f\"Titre: {title}, URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trie le top compétiteurs kaggle en fonction de leur médaille d'or et d'argent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Récupérer les variables d'environnement pour l'authentification\n",
    "kaggle_username = os.getenv('KAGGLE_USERNAME')\n",
    "kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "if not kaggle_username or not kaggle_key:\n",
    "    raise Exception(\n",
    "        \"Les variables d'environnement KAGGLE_USERNAME et KAGGLE_KEY doivent être définies.\")\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_username\n",
    "os.environ['KAGGLE_KEY'] = kaggle_key\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Récupérer une liste d'utilisateurs\n",
    "users = api.users_list(page=1, max_results=300)\n",
    "\n",
    "# Créer une liste pour stocker les informations des utilisateurs\n",
    "user_medals = []\n",
    "\n",
    "for user in users:\n",
    "    # Récupérer le nombre de médailles d'or et d'argent\n",
    "    gold_medals = user.competitionGoldMedals\n",
    "    silver_medals = user.competitionSilverMedals\n",
    "\n",
    "    # Ajouter les informations à la liste\n",
    "    user_medals.append((user.userName, gold_medals, silver_medals))\n",
    "\n",
    "# Trier les utilisateurs par le nombre de médailles d'or, puis par argent\n",
    "sorted_users = sorted(user_medals, key=lambda x: (-x[1], -x[2]))\n",
    "\n",
    "# Afficher les utilisateurs triés\n",
    "for user in sorted_users[:300]:  # Limiter à 300 meilleurs\n",
    "    print(f\"User: {user[0]}, Gold Medals: {user[1]}, Silver Medals: {user[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les 5 grands type d'architecture de réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction création architecture réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Optimisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version avec moins de paramétres afin déavaluer le temps si l'on met tous les metriques\n",
    "dans notre cas nous aurons 2*2*2*2*2*2*2 itérations= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nombre_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m history \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mnombre_epochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping_cb], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Évaluer la performance du modèle\u001b[39;00m\n\u001b[0;32m     55\u001b[0m current_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nombre_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger l'ensemble de données Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "# Normaliser les données\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Modifier les listes d'hyperparamètres pour réduire les itérations\n",
    "taux_apprentissage = [0.01, 0.001]  # réduit à 2 options\n",
    "batch_sizes = [64, 128]  # réduit à 2 options\n",
    "architectures_couches = [[64], [128, 64]]  # réduit à 2 options\n",
    "taux_dropout = [0.2, 0.3]  # réduit à 2 options\n",
    "fonctions_perte = ['categorical_crossentropy',\n",
    "                   'mean_squared_error']  # réduit à 2 options\n",
    "optimiseurs = [keras.optimizers.Adam,\n",
    "               keras.optimizers.SGD]  # réduit à 2 options\n",
    "fonctions_activation = ['relu', 'sigmoid']  # réduit à 2 options\n",
    "\n",
    "meilleurs_resultats = []\n",
    "meilleure_accuracy = 0\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for lr in tqdm(taux_apprentissage):\n",
    "    for batch_size in batch_sizes:\n",
    "        for architecture in architectures_couches:\n",
    "            for dropout in taux_dropout:\n",
    "                for loss_function in fonctions_perte:\n",
    "                    for Optimizer in optimiseurs:\n",
    "                        for activation_function in fonctions_activation:\n",
    "                            iteration_start_time = time.perf_counter()\n",
    "                            # Votre code pour la construction, la compilation et l'entraînement du modèle...\n",
    "                            # Construction et compilation du modèle\n",
    "                            net = keras.models.Sequential()\n",
    "                            net.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "                            for neurones in architecture:\n",
    "                                net.add(keras.layers.Dense(\n",
    "                                    neurones, activation=activation_function))\n",
    "                                net.add(keras.layers.Dropout(dropout))\n",
    "                            net.add(keras.layers.Dense(\n",
    "                                10, activation='softmax'))\n",
    "                            optimizer = Optimizer(learning_rate=lr)\n",
    "                            net.compile(optimizer=optimizer,\n",
    "                                        loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "                            # Callback pour l'arrêt prématuré\n",
    "                            early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "                                patience=5, restore_best_weights=True)\n",
    "\n",
    "                            # Entraînement du modèle\n",
    "                            history = net.fit(X_train, y_train, epochs=nombre_epochs, batch_size=batch_size,\n",
    "                                              validation_split=0.2, callbacks=[early_stopping_cb], verbose=0)\n",
    "\n",
    "                            # Évaluer la performance du modèle\n",
    "                            current_accuracy = max(\n",
    "                                history.history['val_accuracy'])\n",
    "\n",
    "                            # Comparer et stocker le meilleur résultat\n",
    "                            if current_accuracy > meilleure_accuracy:\n",
    "                                meilleure_accuracy = current_accuracy\n",
    "                                meilleurs_resultats = [\n",
    "                                    ('taux_apprentissage', lr),\n",
    "                                    ('batch_size', batch_size),\n",
    "                                    ('architecture', architecture),\n",
    "                                    ('dropout', dropout),\n",
    "                                    ('fonction_perte', loss_function),\n",
    "                                    ('optimiseur', Optimizer.__name__),\n",
    "                                    ('fonction_activation', activation_function),\n",
    "                                    ('accuracy', meilleure_accuracy)\n",
    "                                ]\n",
    "\n",
    "                            iteration_end_time = time.perf_counter()\n",
    "                            print(f\"Temps pour cette itération: {\n",
    "                                  iteration_end_time - iteration_start_time} secondes\")\n",
    "\n",
    "# Votre code pour afficher les meilleurs hyperparamètres...\n",
    "# Afficher les meilleurs hyperparamètres après toutes les itérations\n",
    "print(\"Meilleurs hyperparamètres : \", meilleurs_resultats)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Temps total d'exécution: {end_time - start_time} secondes\")\n",
    "\n",
    "# temps total pour les 128 itération =2h avec 59s en moyenne par itération\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de la complexité d'une fonction\n",
    "\n",
    "Nombre total d'itérations: Calculez le nombre total de combinaisons d'hyperparamètres en multipliant le nombre d'options pour chaque hyperparamètre. Dans votre cas :\n",
    "\n",
    "* taux_apprentissage: 3 options\n",
    "* batch_sizes: 3 options\n",
    "* architectures_couches: 4 options\n",
    "* taux_dropout: 3 options\n",
    "* fonctions_perte: 6 options (7 moins 1 puisque vous avez enlevé une option)\n",
    "* optimiseurs: 7 options\n",
    "* fonctions_activation: 4 options\n",
    "\n",
    "Le nombre total d'itérations est donc 3 x 3 x 4 x 3 x 6 x 7 x 4 = 18 144."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour cette itération: 76.19445170000836 secondes\n",
      "Temps pour cette itération: 114.79364879999775 secondes\n",
      "Temps pour cette itération: 161.74815749999834 secondes\n",
      "Temps pour cette itération: 188.8438976000034 secondes\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Charger l'ensemble de données Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "# Normaliser les données si nécessaire\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Listes d'hyperparamètres\n",
    "nombre_epochs = 50\n",
    "taux_apprentissage = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [64, 128, 256]\n",
    "architectures_couches = [[64], [128], [64, 64], [128, 64]]\n",
    "taux_dropout = [0.2, 0.3, 0.5]\n",
    "#  suppression de sparse_categorical_crossentropy car incompatible avec oneEncoder, attend des catégories\n",
    "fonctions_perte = ['categorical_crossentropy', 'binary_crossentropy', 'mean_squared_error',\n",
    "                   'mean_absolute_error', 'kullback_leibler_divergence', 'poisson', 'cosine_similarity']\n",
    "# fonctions_perte = ['sparse_categorical_crossentropy', 'categorical_crossentropy', 'binary_crossentropy', 'mean_squared_error', 'mean_absolute_error', 'kullback_leibler_divergence', 'poisson', 'cosine_similarity']\n",
    "optimiseurs = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop,\n",
    "               keras.optimizers.Adagrad, keras.optimizers.Adadelta, keras.optimizers.Nadam, keras.optimizers.Ftrl]\n",
    "fonctions_activation = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
    "\n",
    "meilleurs_resultats = []\n",
    "meilleure_accuracy = 0\n",
    "\n",
    "for lr in taux_apprentissage:\n",
    "    for batch_size in batch_sizes:\n",
    "        for architecture in architectures_couches:\n",
    "            for dropout in taux_dropout:\n",
    "                for loss_function in fonctions_perte:\n",
    "                    for Optimizer in optimiseurs:\n",
    "                        for activation_function in fonctions_activation:\n",
    "                            # Construction et compilation du modèle\n",
    "                            net = keras.models.Sequential()\n",
    "                            net.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "                            for neurones in architecture:\n",
    "                                net.add(keras.layers.Dense(\n",
    "                                    neurones, activation=activation_function))\n",
    "                                net.add(keras.layers.Dropout(dropout))\n",
    "                            net.add(keras.layers.Dense(\n",
    "                                10, activation='softmax'))\n",
    "                            optimizer = Optimizer(learning_rate=lr)\n",
    "                            net.compile(optimizer=optimizer,\n",
    "                                        loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "                            # Callback pour l'arrêt prématuré\n",
    "                            early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "                                patience=5, restore_best_weights=True)\n",
    "\n",
    "                            # Entraînement du modèle\n",
    "                            history = net.fit(X_train, y_train, epochs=nombre_epochs, batch_size=batch_size,\n",
    "                                              validation_split=0.2, callbacks=[early_stopping_cb], verbose=0)\n",
    "\n",
    "                            # Évaluer la performance du modèle\n",
    "                            current_accuracy = max(\n",
    "                                history.history['val_accuracy'])\n",
    "\n",
    "                            # Comparer et stocker le meilleur résultat\n",
    "                            if current_accuracy > meilleure_accuracy:\n",
    "                                meilleure_accuracy = current_accuracy\n",
    "                                meilleurs_resultats = [\n",
    "                                    ('taux_apprentissage', lr),\n",
    "                                    ('batch_size', batch_size),\n",
    "                                    ('architecture', architecture),\n",
    "                                    ('dropout', dropout),\n",
    "                                    ('fonction_perte', loss_function),\n",
    "                                    ('optimiseur', Optimizer.__name__),\n",
    "                                    ('fonction_activation', activation_function),\n",
    "                                    ('accuracy', meilleure_accuracy)\n",
    "                                ]\n",
    "                             # , la compilation et l'entraînement du modèle...\n",
    "                            iteration_end_time = time.perf_counter()\n",
    "                            print(f\"Temps pour cette itération: {\n",
    "                                  iteration_end_time - iteration_start_time} secondes\")\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres après toutes les itérations\n",
    "print(\"Meilleurs hyperparamètres : \", meilleurs_resultats)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"Temps d'exécution du script: {duration} secondes\")\n",
    "\n",
    "# # Exemple d'utilisation\n",
    "# dataset = pd.read_csv('your_dataset.csv')\n",
    "# architectures = [[64, 64], [128, 64], [128, 128, 64]]\n",
    "# metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "# evaluate_neural_networks(dataset, 'target_column', architectures, metrics)\n",
    "\n",
    "# Le modele ne donne rien aprées 630 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22,28\t57,21459677\t128\ttemps\t2,034296774\theures\n",
    "25,09\t\t19000\t\t301,9659274\t\n",
    "75,32\t\t\t\t\t\n",
    "72,74\t\t\t\t\t\n",
    "24,49\t\t\t\t\t\n",
    "45,29\t\t\t\t\t\n",
    "78,3\t\t\t\t\t\n",
    "74,6\t\t\t\t\t\n",
    "30,22\t\t\t\t\t\n",
    "24,71\t\t\t\t\t\n",
    "73,71\t\t\t\t\t\n",
    "71,93\t\t\t\t\t\n",
    "25,04\t\t\t\t\t\n",
    "31,52\t\t\t\t\t\n",
    "72,55\t\t\t\t\t\n",
    "73,78\t\t\t\t\t\n",
    "57,43\t\t\t\t\t\n",
    "53,96\t\t\t\t\t\n",
    "103,5\t\t\t\t\t\n",
    "102,5\t\t\t\t\t\n",
    "49,42\t\t\t\t\t\n",
    "46,75\t\t\t\t\t\n",
    "104,9\t\t\t\t\t\n",
    "104,5\t\t\t\t\t\n",
    "38,32\t\t\t\t\t\n",
    "51,48\t\t\t\t\t\n",
    "102,9\t\t\t\t\t\n",
    "103,2\t\t\t\t\t\n",
    "27,85\t\t\t\t\t\n",
    "61,87\t\t\t\t\t\n",
    "102,8\t\t\t\t\t\n",
    "102,2\t\t\t\t\t\n",
    "14,27\t\t\t\t\t\n",
    "24,79\t\t\t\t\t\n",
    "44,37\t\t\t\t\t\n",
    "44,88\t\t\t\t\t\n",
    "11,88\t\t\t\t\t\n",
    "40,14\t\t\t\t\t\n",
    "44,11\t\t\t\t\t\n",
    "44,86\t\t\t\t\t\n",
    "22,48\t\t\t\t\t\n",
    "17,77\t\t\t\t\t\n",
    "45,65\t\t\t\t\t\n",
    "45,66\t\t\t\t\t\n",
    "21,93\t\t\t\t\t\n",
    "34,33\t\t\t\t\t\n",
    "44,75\t\t\t\t\t\n",
    "45,34\t\t\t\t\t\n",
    "23,82\t\t\t\t\t\n",
    "20,99\t\t\t\t\t\n",
    "62,67\t\t\t\t\t\n",
    "62,55\t\t\t\t\t\n",
    "23,56\t\t\t\t\t\n",
    "33,87\t\t\t\t\t\n",
    "62,32\t\t\t\t\t\n",
    "61,13\t\t\t\t\t\n",
    "27,08\t\t\t\t\t\n",
    "49,81\t\t\t\t\t\n",
    "65,26\t\t\t\t\t\n",
    "64,93\t\t\t\t\t\n",
    "29,04\t\t\t\t\t\n",
    "40,2\t\t\t\t\t\n",
    "65,78\t\t\t\t\t\n",
    "64,57\t\t\t\t\t\n",
    "39,61\t\t\t\t\t\n",
    "57,12\t\t\t\t\t\n",
    "72,25\t\t\t\t\t\n",
    "75,28\t\t\t\t\t\n",
    "42,48\t\t\t\t\t\n",
    "81,32\t\t\t\t\t\n",
    "77,13\t\t\t\t\t\n",
    "73,74\t\t\t\t\t\n",
    "40,03\t\t\t\t\t\n",
    "73,1\t\t\t\t\t\n",
    "71,96\t\t\t\t\t\n",
    "71,72\t\t\t\t\t\n",
    "36,84\t\t\t\t\t\n",
    "69,58\t\t\t\t\t\n",
    "72,98\t\t\t\t\t\n",
    "72,11\t\t\t\t\t\n",
    "71,62\t\t\t\t\t\n",
    "98,54\t\t\t\t\t\n",
    "98,6\t\t\t\t\t\n",
    "101,3\t\t\t\t\t\n",
    "57,29\t\t\t\t\t\n",
    "82,53\t\t\t\t\t\n",
    "96,86\t\t\t\t\t\n",
    "97,62\t\t\t\t\t\n",
    "49,86\t\t\t\t\t\n",
    "88,38\t\t\t\t\t\n",
    "94,58\t\t\t\t\t\n",
    "95,99\t\t\t\t\t\n",
    "69,36\t\t\t\t\t\n",
    "104,5\t\t\t\t\t\n",
    "95,28\t\t\t\t\t\n",
    "97,45\t\t\t\t\t\n",
    "34,96\t\t\t\t\t\n",
    "45,75\t\t\t\t\t\n",
    "49,37\t\t\t\t\t\n",
    "43,97\t\t\t\t\t\n",
    "30,87\t\t\t\t\t\n",
    "49,22\t\t\t\t\t\n",
    "44,59\t\t\t\t\t\n",
    "44,04\t\t\t\t\t\n",
    "26,7\t\t\t\t\t\n",
    "50,08\t\t\t\t\t\n",
    "44,05\t\t\t\t\t\n",
    "43,9\t\t\t\t\t\n",
    "26,48\t\t\t\t\t\n",
    "49,75\t\t\t\t\t\n",
    "43,86\t\t\t\t\t\n",
    "43,96\t\t\t\t\t\n",
    "29,41\t\t\t\t\t\n",
    "58,22\t\t\t\t\t\n",
    "57,74\t\t\t\t\t\n",
    "58,28\t\t\t\t\t\n",
    "47,01\t\t\t\t\t\n",
    "60,2\t\t\t\t\t\n",
    "58,74\t\t\t\t\t\n",
    "58,78\t\t\t\t\t\n",
    "31,93\t\t\t\t\t\n",
    "72,56\t\t\t\t\t\n",
    "63,41\t\t\t\t\t\n",
    "59,83\t\t\t\t\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
