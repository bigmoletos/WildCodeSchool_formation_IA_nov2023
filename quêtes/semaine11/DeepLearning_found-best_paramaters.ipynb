{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche d'une fonction d'optimisation d'une architecture d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des differents types de DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les 5 grands types de datasets couramment rencontrés dans le domaine de la science des données :\n",
    "\n",
    "***Données Structurées*** :\n",
    "\n",
    "- Ce sont des données qui ont une organisation fixe, généralement sous forme de tableaux avec des lignes et des colonnes. \n",
    "- Chaque colonne représente une variable et chaque ligne une observation.\n",
    "Exemples : bases de données SQL, feuilles de calcul Excel, CSV.\n",
    "\n",
    "***Données Non Structurées*** :\n",
    "\n",
    "- Contrairement aux données structurées, les données non structurées ne suivent pas un modèle ou un format spécifique. \n",
    "\n",
    "- Elles sont plus complexes à analyser et à traiter.\n",
    "\n",
    "Exemples : texte libre (emails, articles), images, vidéos, enregistrements audio.\n",
    "\n",
    "***Données Semi-Structurées*** :\n",
    "\n",
    "- Ces données ne sont pas organisées dans des tableaux rigides comme les données structurées, mais contiennent néanmoins des marqueurs ou des étiquettes qui séparent différents éléments.\n",
    "Exemples : documents JSON, XML.\n",
    "\n",
    "***Données Temporelles (Time Series)*** :\n",
    "\n",
    "- Il s'agit de données qui sont collectées, enregistrées ou organisées en fonction du temps. \n",
    "- Elles sont utilisées pour analyser les tendances, les prévisions, etc.\n",
    "Exemples : enregistrements boursiers, données météorologiques historiques, séries chronologiques de ventes.\n",
    "\n",
    "***Données Spatiales (ou Géospatiales)*** :\n",
    "\n",
    "- Ce type de données est lié à des informations géographiques ou spatiales. \n",
    "- Elles sont souvent utilisées pour des analyses cartographiques ou de localisation.\n",
    "Exemples : cartes, données GPS, données de localisation de téléphones mobiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing top 5 par type de dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***Données Structurées***\n",
    "Nettoyage des Données :\n",
    "\n",
    "- Gestion des valeurs manquantes : imputation, suppression des lignes/colonnes.\n",
    "- Correction des erreurs et incohérences : valeurs aberrantes, erreurs de saisie.\n",
    "- Transformation des Données :\n",
    "\n",
    "- Normalisation/Standardisation : mise à l'échelle des variables pour une plage commune.\n",
    "- Encodage : conversion des variables catégorielles en numériques (One-Hot Encoding, Label Encoding).\n",
    "- Réduction de Dimensionnalité : Analyse en Composantes Principales (PCA), Sélection de Caractéristiques.\n",
    "\n",
    "2. ***Données Non Structurées***\n",
    "Texte :\n",
    "\n",
    "- Nettoyage : suppression des balises HTML, des caractères spéciaux, et mise en minuscules.\n",
    "- Tokenisation : découpage en mots ou phrases.\n",
    "Suppression des mots vides (stop words), Stemming/Lemmatisation.\n",
    "- Encodage : Bag of Words, TF-IDF, Word Embeddings.\n",
    "\n",
    "*Images et Vidéos* :\n",
    "\n",
    "- Normalisation : ajustement de la taille, mise à l'échelle des valeurs de pixels.\n",
    "- Augmentation des données : rotation, translation, retournement pour augmenter la taille du dataset.\n",
    "- Extraction de caractéristiques : utilisation de réseaux de neurones convolutifs (CNN).\n",
    "\n",
    "*Audio* :\n",
    "\n",
    "Extraction de caractéristiques : spectrogrammes, MFCC (Mel-Frequency Cepstral Coefficients).\n",
    "Réduction du bruit.\n",
    "\n",
    "3. ***Données Semi-Structurées***\n",
    "- Extraction de Caractéristiques :\n",
    "Conversion de formats JSON ou XML en structures tabulaires.\n",
    "Extraction de champs spécifiques et transformation en données structurées pour une analyse plus poussée.\n",
    "\n",
    "4. ***Données Temporelles (Time Series)***\n",
    "Transformation :\n",
    "\n",
    "Découpage en fenêtres temporelles.\n",
    "Création de caractéristiques basées sur le temps : tendances, saisonnalité.\n",
    "- Normalisation :\n",
    "\n",
    "- Standardisation des séries temporelles.\n",
    "- Décomposition :\n",
    "\n",
    "Séparation des composantes tendancielles, saisonnières et résiduelles.\n",
    "\n",
    "5. ***Données Spatiales (ou Géospatiales)***\n",
    "Transformation des Coordonnées :\n",
    "\n",
    "- Conversion des formats de coordonnées, ajustement aux systèmes de référence.\n",
    "- Traitement des Données Raster et Vectorielles :\n",
    "\n",
    "- Nettoyage, simplification des formes, superposition de couches d'informations.\n",
    "- Extraction de Caractéristiques :\n",
    "\n",
    "Création de caractéristiques basées sur la localisation (par exemple, distance à un point d'intérêt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'évaluation du type de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def determine_dataset_type(dataset):\n",
    "    # Vérifie si le dataset est un DataFrame pandas (données structurées)\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # Vérifie la présence de colonnes temporelles\n",
    "        if any(pd.api.types.is_datetime64_any_dtype(dataset[col]) for col in dataset.columns):\n",
    "            return \"Time Series (Temporal Data)\"\n",
    "        # Vérifie la présence de colonnes spatiales (latitude/longitude)\n",
    "        if 'latitude' in dataset.columns and 'longitude' in dataset.columns:\n",
    "            return \"Spatial Data\"\n",
    "        return \"Structured Data\"\n",
    "\n",
    "    # Vérifie si le dataset est une série de textes ou de fichiers (données non structurées)\n",
    "    if isinstance(dataset, (list, pd.Series)) and all(isinstance(x, str) for x in dataset):\n",
    "        return \"Unstructured Data (Text or Files)\"\n",
    "\n",
    "    # Vérifie si le dataset est sous forme de dictionnaires ou de JSON (semi-structuré)\n",
    "    if isinstance(dataset, (dict, list)) and all(isinstance(x, (dict, list)) for x in dataset):\n",
    "        return \"Semi-Structured Data (e.g., JSON)\"\n",
    "\n",
    "    return \"Unknown Type\"\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Remplacez par le chemin de votre fichier de données\n",
    "dataset = pd.read_csv('SpotifyFeatures.csv')\n",
    "\n",
    "print(determine_dataset_type(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iris Dataset\n",
    "Description : Contient des mesures de longueur et largeur de sépales et pétales pour trois espèces d'iris.\n",
    "Type Attendu : Structured Data\n",
    "2. Enron Email Dataset\n",
    "Description : Collection d'environ 500 000 emails provenant de cadres de l'entreprise Enron.\n",
    "Type Attendu : Unstructured Data (Text or Files)\n",
    "3. New York City Taxi Trip Records\n",
    "Description : Contient des enregistrements détaillés des trajets en taxi à New York, y compris les coordonnées de départ et d'arrivée.\n",
    "Type Attendu : Spatial Data\n",
    "4. Bitcoin Historical Data\n",
    "Description : Données historiques sur le prix du Bitcoin, y compris la date et l'heure des transactions.\n",
    "Type Attendu : Time Series (Temporal Data)\n",
    "5. OpenStreetMap Data\n",
    "Description : Contient des données géospatiales sous forme de fichiers XML, y compris des informations sur les routes, les bâtiments, etc.\n",
    "Type Attendu : Semi-Structured Data (e.g., JSON) - Bien que les données OpenStreetMap soient souvent en XML, la fonction pourrait les identifier comme semi-structurées en raison de leur nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iris Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print(determine_dataset_type(iris_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Enron Email Dataset (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Type\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de mails sous forme de liste de chaînes de caractères\n",
    "# Ajoutez des exemples de mails\n",
    "emails = [\"Here is the meeting agenda...\",\n",
    "          \"Please find attached the report...\", ...]\n",
    "print(determine_dataset_type(emails))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. New York City Taxi Trip Records (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Data\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de trajets de taxi avec des coordonnées\n",
    "taxi_data = pd.DataFrame({\n",
    "    'pickup_latitude': [40.761432, 40.644102, ...],  # Ajoutez des coordonnées\n",
    "    'pickup_longitude': [-73.979815, -73.781632, ...],\n",
    "    'dropoff_latitude': [40.641233, 40.729523, ...],\n",
    "    'dropoff_longitude': [-73.958763, -73.991567, ...]\n",
    "})\n",
    "print(determine_dataset_type(taxi_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bitcoin Historical Data (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series (Temporal Data)\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de données historiques Bitcoin\n",
    "btc_data = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2021-01-01', periods=100, freq='D'),\n",
    "    'price': np.random.uniform(30000, 60000, 100)  # Prix aléatoires\n",
    "})\n",
    "print(determine_dataset_type(btc_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. OpenStreetMap Data (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown Type\n"
     ]
    }
   ],
   "source": [
    "# Simuler un dataset de données géospatiales (XML/JSON-like structure)\n",
    "osm_data = [\n",
    "    {\"type\": \"node\", \"id\": 1, \"lat\": 59.941, \"lon\": 30.313},\n",
    "    {\"type\": \"node\", \"id\": 2, \"lat\": 59.942, \"lon\": 30.314},\n",
    "    ...  # Ajoutez des données fictives similaires\n",
    "]\n",
    "print(determine_dataset_type(osm_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing des datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données structurées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def preprocess_structured_data(dataset):\n",
    "    # Prétraitement pour les données structurées\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    dataset = pd.DataFrame(imputer.fit_transform(\n",
    "        dataset), columns=dataset.columns)\n",
    "    scaler = StandardScaler()\n",
    "    dataset = pd.DataFrame(scaler.fit_transform(\n",
    "        dataset), columns=dataset.columns)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données non structurées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def preprocess_unstructured_data(dataset):\n",
    "    # Prétraitement pour les données textuelles non structurées\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    dataset = vectorizer.fit_transform(dataset)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données de type times series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_temporal_data(dataset):\n",
    "    # Prétraitement pour les données temporelles\n",
    "    dataset.fillna(method='ffill', inplace=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pour les Données Spatiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def preprocess_spatial_data(dataset):\n",
    "    # Supposons que dataset est un DataFrame avec des colonnes 'latitude' et 'longitude'\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset[['latitude', 'longitude']] = scaler.fit_transform(\n",
    "        dataset[['latitude', 'longitude']])\n",
    "\n",
    "    # Calcul d'une caractéristique dérivée, par exemple, la distance depuis le centre-ville\n",
    "    city_center = (48.8566, 2.3522)  # Coordonnées de Paris, par exemple\n",
    "    dataset['distance_from_center'] = np.sqrt(\n",
    "        (dataset['latitude'] - city_center[0])**2 + (dataset['longitude'] - city_center[1])**2)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "spatial_data = pd.DataFrame({\n",
    "    'latitude': np.random.uniform(48.8, 48.9, 100),\n",
    "    'longitude': np.random.uniform(2.3, 2.4, 100)\n",
    "})\n",
    "preprocessed_spatial_data = preprocess_spatial_data(spatial_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing pour les Données Semi-Structurées (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def preprocess_json_data(json_data):\n",
    "    # Convertir le JSON en DataFrame\n",
    "    dataset = pd.json_normalize(json_data)\n",
    "\n",
    "    # Gérer les valeurs manquantes\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    dataset = pd.DataFrame(imputer.fit_transform(\n",
    "        dataset), columns=dataset.columns)\n",
    "\n",
    "    # Encoder les variables catégorielles\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_columns = encoder.fit_transform(dataset[['interests']]).toarray()\n",
    "    dataset = dataset.join(pd.DataFrame(\n",
    "        encoded_columns, columns=encoder.get_feature_names_out(['interests'])))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "json_data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"interests\": \"music\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"interests\": \"sports\"},\n",
    "    # ... autres données\n",
    "]\n",
    "preprocessed_json_data = preprocess_json_data(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction globale de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset_type = determine_dataset_type(dataset)\n",
    "\n",
    "    if dataset_type == \"Structured Data\":\n",
    "        return preprocess_structured_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Unstructured Data (Text or Files)\":\n",
    "        return preprocess_unstructured_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Time Series (Temporal Data)\":\n",
    "        return preprocess_temporal_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Spatial Data\":\n",
    "        return preprocess_spatial_data(dataset)\n",
    "\n",
    "    elif dataset_type == \"Semi-Structured Data (e.g., JSON)\":\n",
    "        return preprocess_json_data(dataset)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Type de dataset non reconnu ou non pris en charge pour le prétraitement\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    # Identifier le type de dataset\n",
    "    dataset_type = determine_dataset_type(dataset)\n",
    "\n",
    "    if dataset_type == \"Structured Data\":\n",
    "        # Prétraitement pour les données structurées\n",
    "        # Gérer les valeurs manquantes\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        dataset = pd.DataFrame(imputer.fit_transform(\n",
    "            dataset), columns=dataset.columns)\n",
    "\n",
    "        # Encoder les variables catégorielles\n",
    "        dataset = pd.get_dummies(dataset)\n",
    "\n",
    "        # Normaliser les données\n",
    "        scaler = StandardScaler()\n",
    "        dataset = pd.DataFrame(scaler.fit_transform(\n",
    "            dataset), columns=dataset.columns)\n",
    "\n",
    "    elif dataset_type == \"Unstructured Data (Text or Files)\":\n",
    "        # Prétraitement pour les données textuelles non structurées\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        dataset = vectorizer.fit_transform(dataset)\n",
    "\n",
    "    elif dataset_type == \"Time Series (Temporal Data)\":\n",
    "        # Prétraitement pour les données temporelles\n",
    "        # Remplir les valeurs manquantes\n",
    "        dataset.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    elif dataset_type == \"Spatial Data\":\n",
    "        # Prétraitement pour les données spatiales\n",
    "        # Supposer que les données sont déjà sous forme numérique appropriée pour l'analyse\n",
    "        pass\n",
    "\n",
    "    elif dataset_type == \"Semi-Structured Data (e.g., JSON)\":\n",
    "        # Prétraitement pour les données semi-structurées\n",
    "        # Convertir en DataFrame (si nécessaire) et appliquer le prétraitement comme pour les données structurées\n",
    "        # Cette étape dépendra fortement de la structure spécifique des données semi-structurées\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Type de dataset non reconnu ou non pris en charge pour le prétraitement\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Utiliser la fonction sur un dataset spécifique\n",
    "# dataset = ...  # Charger ou définir le dataset\n",
    "# preprocessed_dataset = preprocess_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place de l'API kaggle pour receuillir l'architecture des 300 meilleurs competiteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.3.tar.gz (84 kB)\n",
      "     ---------------------------------------- 0.0/84.5 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/84.5 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 30.7/84.5 kB 262.6 kB/s eta 0:00:01\n",
      "     --------------------------- ---------- 61.4/84.5 kB 409.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 84.5/84.5 kB 526.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.1)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.3-py3-none-any.whl size=111932 sha256=c6988c827d645293ed437aca1840181b89471727652e0f8d5b000bb07c669093\n",
      "  Stored in directory: c:\\users\\romar\\appdata\\local\\pip\\cache\\wheels\\c5\\94\\5b\\08d5bb9b7b78401fa26da264ef32d72bfbd9cb74641c65169b\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script kaggle.exe is installed in 'C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code utilisant les variables globales de python, ne fonctionne qu'avec la version courante de python, et donc pas avec tensorflow qui utilise une autre version de python.\n",
    "import os\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "print(os.environ['KAGGLE_USERNAME'])\n",
    "# print(os.environ[KAGGLE_USERNAME])\n",
    "print(os.environ['KAGGLE_KEY'])\n",
    "\n",
    "\n",
    "# Utilisation des variables d'environnement pour l'authentification\n",
    "# Remplacez par votre nom d'utilisateur Kaggle\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = 'your_username'\n",
    "\n",
    "os.environ['KAGGLE_KEY'] = 'your_key'  # Remplacez par votre clé API Kaggle\n",
    "\n",
    "\n",
    "api = KaggleApi()\n",
    "\n",
    "api.authenticate()\n",
    "\n",
    "\n",
    "# Exemple pour récupérer une liste d'utilisateurs\n",
    "\n",
    "# Note : Cette méthode ne garantit pas d'obtenir les 300 meilleurs compétiteurs\n",
    "\n",
    "users = api.users_list(page=1, max_results=300)\n",
    "\n",
    "for user in users:\n",
    "\n",
    "    print(user.userName, user.performanceTier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la requête GET : 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la page de la compétition Kaggle\n",
    "competition_url = \"https://www.kaggle.com/c/nom-de-la-competition/participants\"  # Remplacez par l'URL de la compétition\n",
    "\n",
    "# Envoyer une requête GET pour récupérer la page HTML\n",
    "response = requests.get(competition_url)\n",
    "\n",
    "# Vérifier si la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Analyser la page HTML avec BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Trouver la section contenant la liste des compétiteurs\n",
    "    competitors_section = soup.find(\"div\", class_=\"ParticipantsHeader_title_1b6Gh\")\n",
    "\n",
    "    if competitors_section:\n",
    "        # Extraire la liste des compétiteurs\n",
    "        competitors_list = competitors_section.find_next(\"ul\")\n",
    "\n",
    "        # Afficher la liste des compétiteurs\n",
    "        for competitor in competitors_list.find_all(\"li\"):\n",
    "            competitor_name = competitor.text.strip()\n",
    "            print(f\"Nom du compétiteur : {competitor_name}\")\n",
    "    else:\n",
    "        print(\"La section des compétiteurs n'a pas été trouvée sur la page.\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la requête GET : {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "competition :\n",
      "https://www.kaggle.com/competitions/blood-vessel-segmentation \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KaggleApi' object has no attribute 'competition_list_teams_with_http_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcompetition :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcompetition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Obtenez la liste des compétiteurs de la compétition\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m competitors, _, _ \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mcompetition_list_teams_with_http_info(competition)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Affichez la liste des compétiteurs\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m competitor \u001b[38;5;129;01min\u001b[39;00m competitors:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KaggleApi' object has no attribute 'competition_list_teams_with_http_info'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from tqdm import tqdm  # Importez tqdm\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# # Obtenez la liste des compétiteurs de la compétition\n",
    "# competitors = api.competition_list_teams(competition_name)\n",
    "\n",
    "# for competitor in competitors:\n",
    "#     print(competitor.userName, competitor.performanceTier)\n",
    "\n",
    "\n",
    "\n",
    "# Lister les competitions disponibles\n",
    "competitions = api.competitions_list()\n",
    "for competition in tqdm(competitions):\n",
    "    print(f\"\\ncompetition :\\n{competition} \\n\")\n",
    "    # Obtenez la liste des compétiteurs de la compétition\n",
    "    competitors, _, _ = api.competition_list_teams_with_http_info(competition)\n",
    "    # Affichez la liste des compétiteurs\n",
    "    for competitor in competitors:\n",
    "        print(f\"Nom du compétiteur : {competitor['name']}\")\n",
    "\n",
    "# Lister les datasets disponibles\n",
    "datasets = api.datasets_list()\n",
    "# print(f\"\\n datasets:\\n{datasets[0:10]} \\n\")\n",
    "# info des 10 premiers\n",
    "for dataset in tqdm(datasets[:10]):\n",
    "    title = dataset.get('title', 'Titre non disponible')\n",
    "    url = dataset.get('url', 'URL non disponible')\n",
    "    print(f\" Dataset Titre: {title}, URL: {url}\")\n",
    "\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "    title = dataset['titleNullable'] if 'titleNullable' in dataset else 'Titre non disponible'\n",
    "    url = dataset['urlNullable'] if 'urlNullable' in dataset else 'URL non disponible'\n",
    "    print(f\" DAtaset Titre: {title}, URL: {url}\")\n",
    "\n",
    "kernels = api.kernels_list()\n",
    "# print(f\"\\n kernels:\\n{kernels[:10]} \\n\")\n",
    "for kernel in kernel_objects:\n",
    "    title = kernel.title\n",
    "    author = kernel.author.get('userName', 'Auteur non disponible')\n",
    "    url = kernel.url\n",
    "    print(f\"Titre: {title}, Auteur: {author}, URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "api :\n",
      "<kaggle.api.kaggle_api_extended.KaggleApi object at 0x00000177BA716450>,api.authenticate(): None \n",
      "\n",
      "Titre: Jobs and Salaries in Data Science, URL: https://www.kaggle.com/datasets/hummaamqaasim/jobs-in-data\n",
      "Titre: Apple Quality, URL: https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality\n",
      "Titre: 57,650 Spotify Songs, URL: https://www.kaggle.com/datasets/joebeachcapital/57651-spotify-songs\n",
      "Titre: Gold Stock Prices, URL: https://www.kaggle.com/datasets/sahilwagh/gold-stock-prices\n",
      "Titre: Heart Attack Prediction, URL: https://www.kaggle.com/datasets/juledz/heart-attack-prediction\n",
      "Titre: TATA Motors Limited - Stock Prices (2006-2024), URL: https://www.kaggle.com/datasets/raunakpandey030/tata-motors-limited-stock-prices-2006-2024\n",
      "Titre: New York Housing Market, URL: https://www.kaggle.com/datasets/nelgiriyewithana/new-york-housing-market\n",
      "Titre: IMDB Movie Dataset Till Dec-2023, URL: https://www.kaggle.com/datasets/kianindeed/imdb-movie-dataset-dec-2023\n",
      "Titre: NFL Team Data 2003-2023, URL: https://www.kaggle.com/datasets/nickcantalupa/nfl-team-data-2003-2023\n",
      "Titre: Sales Store overview, URL: https://www.kaggle.com/datasets/fekihmea/sales-store-overview\n",
      "Titre: DAIGT-V4-TRAIN-DATASET, URL: https://www.kaggle.com/datasets/thedrcat/daigt-v4-train-dataset\n",
      "Titre: Car Sales Report, URL: https://www.kaggle.com/datasets/missionjee/car-sales-report\n",
      "Titre: Brain-Spectrograms, URL: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n",
      "Titre: International Student Demographics, URL: https://www.kaggle.com/datasets/webdevbadger/international-student-demographics\n",
      "Titre: Student Exam Performance Prediction, URL: https://www.kaggle.com/datasets/mrsimple07/student-exam-performance-prediction\n",
      "Titre: Energy-consumption-prediction, URL: https://www.kaggle.com/datasets/mrsimple07/energy-consumption-prediction\n",
      "Titre: World data population, URL: https://www.kaggle.com/datasets/tanishqdublish/world-data-population\n",
      "Titre: Glassdoor Data science Jobs - 2024, URL: https://www.kaggle.com/datasets/kuralamuthan300/glassdoor-data-science-jobs\n",
      "Titre: Depression and Mental Health Data Analysis, URL: https://www.kaggle.com/datasets/shashwatwork/depression-and-mental-health-data-analysis\n",
      "Titre: Data Science Salaries 2024, URL: https://www.kaggle.com/datasets/sazidthe1/data-science-salaries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# print(f\"\\napi :\\n{api},api.authenticate(): {api.authenticate()} \\n\")\n",
    "\n",
    "# Exemple pour récupérer une liste d'utilisateurs\n",
    "# Note : Cette méthode ne garantit pas d'obtenir les 300 meilleurs compétiteurs\n",
    "# users = api.users_list(page=1, max_results=300)\n",
    "# for user in users:\n",
    "#     print(user.userName, user.performanceTier)\n",
    "# Lister les datasets disponibles\n",
    "# Lister les datasets disponibles\n",
    "datasets = api.datasets_list()\n",
    "# print(f\"\\n datasets:\\n{datasets} \\n\")\n",
    "for dataset in datasets:\n",
    "    title = dataset['titleNullable'] if 'titleNullable' in dataset else 'Titre non disponible'\n",
    "    url = dataset['urlNullable'] if 'urlNullable' in dataset else 'URL non disponible'\n",
    "    print(f\"Titre: {title}, URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trie le top compétiteurs kaggle en fonction de leur médaille d'or et d'argent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Récupérer les variables d'environnement pour l'authentification\n",
    "kaggle_username = os.getenv('KAGGLE_USERNAME')\n",
    "kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "if not kaggle_username or not kaggle_key:\n",
    "    raise Exception(\n",
    "        \"Les variables d'environnement KAGGLE_USERNAME et KAGGLE_KEY doivent être définies.\")\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_username\n",
    "os.environ['KAGGLE_KEY'] = kaggle_key\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Récupérer une liste d'utilisateurs\n",
    "users = api.users_list(page=1, max_results=300)\n",
    "\n",
    "# Créer une liste pour stocker les informations des utilisateurs\n",
    "user_medals = []\n",
    "\n",
    "for user in users:\n",
    "    # Récupérer le nombre de médailles d'or et d'argent\n",
    "    gold_medals = user.competitionGoldMedals\n",
    "    silver_medals = user.competitionSilverMedals\n",
    "\n",
    "    # Ajouter les informations à la liste\n",
    "    user_medals.append((user.userName, gold_medals, silver_medals))\n",
    "\n",
    "# Trier les utilisateurs par le nombre de médailles d'or, puis par argent\n",
    "sorted_users = sorted(user_medals, key=lambda x: (-x[1], -x[2]))\n",
    "\n",
    "# Afficher les utilisateurs triés\n",
    "for user in sorted_users[:300]:  # Limiter à 300 meilleurs\n",
    "    print(f\"User: {user[0]}, Gold Medals: {user[1]}, Silver Medals: {user[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les 5 grands type d'architecture de réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction création architecture réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Optimisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version avec moins de paramétres afin déavaluer le temps si l'on met tous les metriques\n",
    "dans notre cas nous aurons 2*2*2*2*2*2*2 itérations= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nombre_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m history \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mnombre_epochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping_cb], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Évaluer la performance du modèle\u001b[39;00m\n\u001b[0;32m     55\u001b[0m current_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nombre_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger l'ensemble de données Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "# Normaliser les données\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Modifier les listes d'hyperparamètres pour réduire les itérations\n",
    "taux_apprentissage = [0.01, 0.001]  # réduit à 2 options\n",
    "batch_sizes = [64, 128]  # réduit à 2 options\n",
    "architectures_couches = [[64], [128, 64]]  # réduit à 2 options\n",
    "taux_dropout = [0.2, 0.3]  # réduit à 2 options\n",
    "fonctions_perte = ['categorical_crossentropy',\n",
    "                   'mean_squared_error']  # réduit à 2 options\n",
    "optimiseurs = [keras.optimizers.Adam,\n",
    "               keras.optimizers.SGD]  # réduit à 2 options\n",
    "fonctions_activation = ['relu', 'sigmoid']  # réduit à 2 options\n",
    "\n",
    "meilleurs_resultats = []\n",
    "meilleure_accuracy = 0\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for lr in tqdm(taux_apprentissage):\n",
    "    for batch_size in batch_sizes:\n",
    "        for architecture in architectures_couches:\n",
    "            for dropout in taux_dropout:\n",
    "                for loss_function in fonctions_perte:\n",
    "                    for Optimizer in optimiseurs:\n",
    "                        for activation_function in fonctions_activation:\n",
    "                            iteration_start_time = time.perf_counter()\n",
    "                            # Votre code pour la construction, la compilation et l'entraînement du modèle...\n",
    "                            # Construction et compilation du modèle\n",
    "                            net = keras.models.Sequential()\n",
    "                            net.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "                            for neurones in architecture:\n",
    "                                net.add(keras.layers.Dense(\n",
    "                                    neurones, activation=activation_function))\n",
    "                                net.add(keras.layers.Dropout(dropout))\n",
    "                            net.add(keras.layers.Dense(\n",
    "                                10, activation='softmax'))\n",
    "                            optimizer = Optimizer(learning_rate=lr)\n",
    "                            net.compile(optimizer=optimizer,\n",
    "                                        loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "                            # Callback pour l'arrêt prématuré\n",
    "                            early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "                                patience=5, restore_best_weights=True)\n",
    "\n",
    "                            # Entraînement du modèle\n",
    "                            history = net.fit(X_train, y_train, epochs=nombre_epochs, batch_size=batch_size,\n",
    "                                              validation_split=0.2, callbacks=[early_stopping_cb], verbose=0)\n",
    "\n",
    "                            # Évaluer la performance du modèle\n",
    "                            current_accuracy = max(\n",
    "                                history.history['val_accuracy'])\n",
    "\n",
    "                            # Comparer et stocker le meilleur résultat\n",
    "                            if current_accuracy > meilleure_accuracy:\n",
    "                                meilleure_accuracy = current_accuracy\n",
    "                                meilleurs_resultats = [\n",
    "                                    ('taux_apprentissage', lr),\n",
    "                                    ('batch_size', batch_size),\n",
    "                                    ('architecture', architecture),\n",
    "                                    ('dropout', dropout),\n",
    "                                    ('fonction_perte', loss_function),\n",
    "                                    ('optimiseur', Optimizer.__name__),\n",
    "                                    ('fonction_activation', activation_function),\n",
    "                                    ('accuracy', meilleure_accuracy)\n",
    "                                ]\n",
    "\n",
    "                            iteration_end_time = time.perf_counter()\n",
    "                            print(f\"Temps pour cette itération: {\n",
    "                                  iteration_end_time - iteration_start_time} secondes\")\n",
    "\n",
    "# Votre code pour afficher les meilleurs hyperparamètres...\n",
    "# Afficher les meilleurs hyperparamètres après toutes les itérations\n",
    "print(\"Meilleurs hyperparamètres : \", meilleurs_resultats)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Temps total d'exécution: {end_time - start_time} secondes\")\n",
    "\n",
    "# temps total pour les 128 itération =2h avec 59s en moyenne par itération\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de la complexité d'une fonction\n",
    "\n",
    "Nombre total d'itérations: Calculez le nombre total de combinaisons d'hyperparamètres en multipliant le nombre d'options pour chaque hyperparamètre. Dans votre cas :\n",
    "\n",
    "* taux_apprentissage: 3 options\n",
    "* batch_sizes: 3 options\n",
    "* architectures_couches: 4 options\n",
    "* taux_dropout: 3 options\n",
    "* fonctions_perte: 6 options (7 moins 1 puisque vous avez enlevé une option)\n",
    "* optimiseurs: 7 options\n",
    "* fonctions_activation: 4 options\n",
    "\n",
    "Le nombre total d'itérations est donc 3 x 3 x 4 x 3 x 6 x 7 x 4 = 18 144."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps pour cette itération: 76.19445170000836 secondes\n",
      "Temps pour cette itération: 114.79364879999775 secondes\n",
      "Temps pour cette itération: 161.74815749999834 secondes\n",
      "Temps pour cette itération: 188.8438976000034 secondes\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Charger l'ensemble de données Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "# Normaliser les données si nécessaire\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Listes d'hyperparamètres\n",
    "nombre_epochs = 50\n",
    "taux_apprentissage = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [64, 128, 256]\n",
    "architectures_couches = [[64], [128], [64, 64], [128, 64]]\n",
    "taux_dropout = [0.2, 0.3, 0.5]\n",
    "#  suppression de sparse_categorical_crossentropy car incompatible avec oneEncoder, attend des catégories\n",
    "fonctions_perte = ['categorical_crossentropy', 'binary_crossentropy', 'mean_squared_error',\n",
    "                   'mean_absolute_error', 'kullback_leibler_divergence', 'poisson', 'cosine_similarity']\n",
    "# fonctions_perte = ['sparse_categorical_crossentropy', 'categorical_crossentropy', 'binary_crossentropy', 'mean_squared_error', 'mean_absolute_error', 'kullback_leibler_divergence', 'poisson', 'cosine_similarity']\n",
    "optimiseurs = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop,\n",
    "               keras.optimizers.Adagrad, keras.optimizers.Adadelta, keras.optimizers.Nadam, keras.optimizers.Ftrl]\n",
    "fonctions_activation = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
    "\n",
    "meilleurs_resultats = []\n",
    "meilleure_accuracy = 0\n",
    "\n",
    "for lr in taux_apprentissage:\n",
    "    for batch_size in batch_sizes:\n",
    "        for architecture in architectures_couches:\n",
    "            for dropout in taux_dropout:\n",
    "                for loss_function in fonctions_perte:\n",
    "                    for Optimizer in optimiseurs:\n",
    "                        for activation_function in fonctions_activation:\n",
    "                            # Construction et compilation du modèle\n",
    "                            net = keras.models.Sequential()\n",
    "                            net.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "                            for neurones in architecture:\n",
    "                                net.add(keras.layers.Dense(\n",
    "                                    neurones, activation=activation_function))\n",
    "                                net.add(keras.layers.Dropout(dropout))\n",
    "                            net.add(keras.layers.Dense(\n",
    "                                10, activation='softmax'))\n",
    "                            optimizer = Optimizer(learning_rate=lr)\n",
    "                            net.compile(optimizer=optimizer,\n",
    "                                        loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "                            # Callback pour l'arrêt prématuré\n",
    "                            early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "                                patience=5, restore_best_weights=True)\n",
    "\n",
    "                            # Entraînement du modèle\n",
    "                            history = net.fit(X_train, y_train, epochs=nombre_epochs, batch_size=batch_size,\n",
    "                                              validation_split=0.2, callbacks=[early_stopping_cb], verbose=0)\n",
    "\n",
    "                            # Évaluer la performance du modèle\n",
    "                            current_accuracy = max(\n",
    "                                history.history['val_accuracy'])\n",
    "\n",
    "                            # Comparer et stocker le meilleur résultat\n",
    "                            if current_accuracy > meilleure_accuracy:\n",
    "                                meilleure_accuracy = current_accuracy\n",
    "                                meilleurs_resultats = [\n",
    "                                    ('taux_apprentissage', lr),\n",
    "                                    ('batch_size', batch_size),\n",
    "                                    ('architecture', architecture),\n",
    "                                    ('dropout', dropout),\n",
    "                                    ('fonction_perte', loss_function),\n",
    "                                    ('optimiseur', Optimizer.__name__),\n",
    "                                    ('fonction_activation', activation_function),\n",
    "                                    ('accuracy', meilleure_accuracy)\n",
    "                                ]\n",
    "                             # , la compilation et l'entraînement du modèle...\n",
    "                            iteration_end_time = time.perf_counter()\n",
    "                            print(f\"Temps pour cette itération: {\n",
    "                                  iteration_end_time - iteration_start_time} secondes\")\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres après toutes les itérations\n",
    "print(\"Meilleurs hyperparamètres : \", meilleurs_resultats)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"Temps d'exécution du script: {duration} secondes\")\n",
    "\n",
    "# # Exemple d'utilisation\n",
    "# dataset = pd.read_csv('your_dataset.csv')\n",
    "# architectures = [[64, 64], [128, 64], [128, 128, 64]]\n",
    "# metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "# evaluate_neural_networks(dataset, 'target_column', architectures, metrics)\n",
    "\n",
    "# Le modele ne donne rien aprées 630 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22,28\t57,21459677\t128\ttemps\t2,034296774\theures\n",
    "25,09\t\t19000\t\t301,9659274\t\n",
    "75,32\t\t\t\t\t\n",
    "72,74\t\t\t\t\t\n",
    "24,49\t\t\t\t\t\n",
    "45,29\t\t\t\t\t\n",
    "78,3\t\t\t\t\t\n",
    "74,6\t\t\t\t\t\n",
    "30,22\t\t\t\t\t\n",
    "24,71\t\t\t\t\t\n",
    "73,71\t\t\t\t\t\n",
    "71,93\t\t\t\t\t\n",
    "25,04\t\t\t\t\t\n",
    "31,52\t\t\t\t\t\n",
    "72,55\t\t\t\t\t\n",
    "73,78\t\t\t\t\t\n",
    "57,43\t\t\t\t\t\n",
    "53,96\t\t\t\t\t\n",
    "103,5\t\t\t\t\t\n",
    "102,5\t\t\t\t\t\n",
    "49,42\t\t\t\t\t\n",
    "46,75\t\t\t\t\t\n",
    "104,9\t\t\t\t\t\n",
    "104,5\t\t\t\t\t\n",
    "38,32\t\t\t\t\t\n",
    "51,48\t\t\t\t\t\n",
    "102,9\t\t\t\t\t\n",
    "103,2\t\t\t\t\t\n",
    "27,85\t\t\t\t\t\n",
    "61,87\t\t\t\t\t\n",
    "102,8\t\t\t\t\t\n",
    "102,2\t\t\t\t\t\n",
    "14,27\t\t\t\t\t\n",
    "24,79\t\t\t\t\t\n",
    "44,37\t\t\t\t\t\n",
    "44,88\t\t\t\t\t\n",
    "11,88\t\t\t\t\t\n",
    "40,14\t\t\t\t\t\n",
    "44,11\t\t\t\t\t\n",
    "44,86\t\t\t\t\t\n",
    "22,48\t\t\t\t\t\n",
    "17,77\t\t\t\t\t\n",
    "45,65\t\t\t\t\t\n",
    "45,66\t\t\t\t\t\n",
    "21,93\t\t\t\t\t\n",
    "34,33\t\t\t\t\t\n",
    "44,75\t\t\t\t\t\n",
    "45,34\t\t\t\t\t\n",
    "23,82\t\t\t\t\t\n",
    "20,99\t\t\t\t\t\n",
    "62,67\t\t\t\t\t\n",
    "62,55\t\t\t\t\t\n",
    "23,56\t\t\t\t\t\n",
    "33,87\t\t\t\t\t\n",
    "62,32\t\t\t\t\t\n",
    "61,13\t\t\t\t\t\n",
    "27,08\t\t\t\t\t\n",
    "49,81\t\t\t\t\t\n",
    "65,26\t\t\t\t\t\n",
    "64,93\t\t\t\t\t\n",
    "29,04\t\t\t\t\t\n",
    "40,2\t\t\t\t\t\n",
    "65,78\t\t\t\t\t\n",
    "64,57\t\t\t\t\t\n",
    "39,61\t\t\t\t\t\n",
    "57,12\t\t\t\t\t\n",
    "72,25\t\t\t\t\t\n",
    "75,28\t\t\t\t\t\n",
    "42,48\t\t\t\t\t\n",
    "81,32\t\t\t\t\t\n",
    "77,13\t\t\t\t\t\n",
    "73,74\t\t\t\t\t\n",
    "40,03\t\t\t\t\t\n",
    "73,1\t\t\t\t\t\n",
    "71,96\t\t\t\t\t\n",
    "71,72\t\t\t\t\t\n",
    "36,84\t\t\t\t\t\n",
    "69,58\t\t\t\t\t\n",
    "72,98\t\t\t\t\t\n",
    "72,11\t\t\t\t\t\n",
    "71,62\t\t\t\t\t\n",
    "98,54\t\t\t\t\t\n",
    "98,6\t\t\t\t\t\n",
    "101,3\t\t\t\t\t\n",
    "57,29\t\t\t\t\t\n",
    "82,53\t\t\t\t\t\n",
    "96,86\t\t\t\t\t\n",
    "97,62\t\t\t\t\t\n",
    "49,86\t\t\t\t\t\n",
    "88,38\t\t\t\t\t\n",
    "94,58\t\t\t\t\t\n",
    "95,99\t\t\t\t\t\n",
    "69,36\t\t\t\t\t\n",
    "104,5\t\t\t\t\t\n",
    "95,28\t\t\t\t\t\n",
    "97,45\t\t\t\t\t\n",
    "34,96\t\t\t\t\t\n",
    "45,75\t\t\t\t\t\n",
    "49,37\t\t\t\t\t\n",
    "43,97\t\t\t\t\t\n",
    "30,87\t\t\t\t\t\n",
    "49,22\t\t\t\t\t\n",
    "44,59\t\t\t\t\t\n",
    "44,04\t\t\t\t\t\n",
    "26,7\t\t\t\t\t\n",
    "50,08\t\t\t\t\t\n",
    "44,05\t\t\t\t\t\n",
    "43,9\t\t\t\t\t\n",
    "26,48\t\t\t\t\t\n",
    "49,75\t\t\t\t\t\n",
    "43,86\t\t\t\t\t\n",
    "43,96\t\t\t\t\t\n",
    "29,41\t\t\t\t\t\n",
    "58,22\t\t\t\t\t\n",
    "57,74\t\t\t\t\t\n",
    "58,28\t\t\t\t\t\n",
    "47,01\t\t\t\t\t\n",
    "60,2\t\t\t\t\t\n",
    "58,74\t\t\t\t\t\n",
    "58,78\t\t\t\t\t\n",
    "31,93\t\t\t\t\t\n",
    "72,56\t\t\t\t\t\n",
    "63,41\t\t\t\t\t\n",
    "59,83\t\t\t\t\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
