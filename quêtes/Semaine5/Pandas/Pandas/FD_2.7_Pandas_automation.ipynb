{"cells":[{"cell_type":"markdown","metadata":{"id":"CZBFjNu331pr"},"source":["# Function to automatic merge and fill missing values\n"]},{"cell_type":"markdown","metadata":{"id":"5sBNQ8Wl39Pk"},"source":["\n","## Create the function\n","You have to create a function which take 2 arguments :\n","- argument 1 : DataFrame main\n","- argument 2 : DataFrame opinion\n","\n","Your function will return a new DataFrame which is the left merge of both DataFrames, and which has no missing values (so your function has to fill them), add new columns (flag, MONTH and DAY), and OPINION should be translated.\n","\n","So you have to copy each useful code from the previous quests, and integrate it into a function. **This should achieve the same results as the previous quests.**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"uy9gkxw44znE"},"outputs":[{"name":"stdout","output_type":"stream","text":["         DATE  MAX_TEMPERATURE_C  MIN_TEMPERATURE_C  WINDSPEED_MAX_KMH  \\\n","0  2018-01-01                 12                  8                 61   \n","1  2018-01-02                 13                  6                 26   \n","2  2018-01-03                 15                 10                 40   \n","3  2018-01-04                 14                 11                 45   \n","4  2018-01-05                 12                  7                 21   \n","\n","   TEMPERATURE_MORNING_C  TEMPERATURE_NOON_C  TEMPERATURE_EVENING_C  \\\n","0                      9                  11                      8   \n","1                      8                  12                     13   \n","2                     11                  12                     10   \n","3                     14                  14                     11   \n","4                     10                  11                      8   \n","\n","   PRECIP_TOTAL_DAY_MM  HUMIDITY_MAX_PERCENT  VISIBILITY_AVG_KM  \\\n","0                  8.9                    79              9.500   \n","1                  0.6                    96              9.000   \n","2                  5.5                    82              8.500   \n","3                  0.0                    89             10.000   \n","4                  1.5                    85              9.875   \n","\n","   PRESSURE_MAX_MB  CLOUDCOVER_AVG_PERCENT  HEATINDEX_MAX_C  DEWPOINT_MAX_C  \\\n","0             1018                  41.750               12               8   \n","1             1020                  87.875               13              12   \n","2             1017                  91.500               15              13   \n","3             1011                  90.125               14              12   \n","4             1005                  62.375               12              10   \n","\n","   WINDTEMP_MAX_C  \n","0               7  \n","1               6  \n","2               7  \n","3              10  \n","4               7  \n","         date  WEATHER_CODE_EVENING  TOTAL_SNOW_MM  UV_INDEX  SUNHOUR OPINION\n","0  2018-01-01                   113              0         3      5.1     bad\n","1  2018-03-12                   119              0         2      8.8     bad\n","2  2018-03-09                   116              0         3     10.2     bad\n","3  2018-10-07                   122              0         1      5.6     bad\n","4  2018-06-18                   119              0         1     12.9     bad\n"]}],"source":["import pandas as pd\n","link_main = \"https://raw.githubusercontent.com/WildCodeSchool/data-training-resources/main/quests/weather_main_2018.csv\"\n","link_opinion = \"https://raw.githubusercontent.com/WildCodeSchool/data-training-resources/main/quests/weather_opinion_2018.csv\"\n","df_main = pd.read_csv(link_main)\n","df_opinion = pd.read_csv(link_opinion)\n","print(df_main.head())\n","print(df_opinion.head())"]},{"cell_type":"markdown","metadata":{},"source":["## INFOS SUR LE DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# INFOS SUR LE DATASET\n","# Have a look at the dataset first\n","from pandas.core.arrays.categorical import factorize_from_iterable\n","df = airbnb\n","df.info()\n","df.describe()\n","print(\"taille: \", df.size, \"valeurs.\", \" \\nDimensions:\", df.shape)\n","print(\"nb na:\", df.isna().all().size, \"nb null:\", df.isnull().all().size)\n","print(\"liste nom des colonnes: \\n\", df.columns)\n","print(\"liste colonnes NA: \\n\", df[df.isna()].columns)\n","df[df.isna()].head(5)\n","# print(\"liste nom des valeurs uniques par ligne: \\n\", df[df.iloc[:,1:]].value.unique())\n","listes_colonnes = ['Invoice ID', 'Branch', 'City', 'Customer type', 'Gender',\n","                   'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Total', 'Date',\n","                   'Time', 'Payment', 'cogs', 'gross margin percentage', 'gross income',\n","                   'Rating']\n","# on retire les colonnes date time et invoice id\n","listes_colonnes2 = ['Branch', 'City', 'Customer type', 'Gender',\n","                    'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Total',\n","                    'Payment', 'cogs', 'gross margin percentage', 'gross income',\n","                    'Rating']\n","for col in listes_colonnes2:\n","    if df[col].dtype == 'object' and col != 'Date' and col != 'Time':\n","\n","        print(f\"Valeurs uniques pour {col} : \\n\", df[col].unique())\n","# ONLY FEMALE\n","# Select a dataset with only female customers. Call it f_customers and keep only the 'Invoice ID', 'Unit price', and 'Quantity' columns.\n","# Check the shape of this subset\n","\n","f_customers = df.loc[df['Gender'] == 'Female',\n","                     ['Invoice ID', 'Unit price', 'Quantity']]\n","f_customers.shape\n","f_customers.head(3)\n","# Select a dataset with only male customers. Call it m_customer and keep only the 'Invoice ID', 'Unit price', and 'Quantity' columns.\n","# Check the shape of this subset\n","m_customers = df.loc[df['Gender'] == 'Male',\n","                     ['Invoice ID', 'Unit price', 'Quantity']]\n","m_customers.shape\n","# Now concat the 2 datasets. Before you start coding, what is the final shape of this dataset? Why?\n","# on s'attend à un shape de (1000,3)\n","\n","\n","# Make sure to check if you are correct in your assumption.\n","\n","fm_customers = pd.concat([f_customers, m_customers], axis=0)\n","fm_customers.shape\n","# fm_customers.head(3)\n","# Select a dataset with only male customers. Call it m_customer_2 and keep only the 'Date', 'Time', and 'Payment' columns.\n","# Check the shape of this subset\n","m_customers_2 = df.loc[df['Gender'] == 'Male',\n","                       ['Date', 'Time', 'Payment']]\n","m_customers_2.shape\n","# Now concat the datasets m_customer and m_customer_2. Before you start coding, what is the final shape of this dataset? Why?\n","#  j'attends un shape de (499,6)\n","\n","\n","# Make sure to check if you are correct in your assumption.\n","\n","\n","pd.concat([m_customers, m_customers_2], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njsItedL3utQ"},"outputs":[],"source":["def merge_and_fill(DataFrameMain, DataFrameOpinion):\n","  # MERGE\n","    # Collect both DataFrames each year\n","    year = str(var_year)\n","    link_main = \"https://raw.githubusercontent.com/WildCodeSchool/data-training-resources/main/quests/weather_main_\" + year + \".csv\"\n","    link_opinion = \"https://raw.githubusercontent.com/WildCodeSchool/data-training-resources/main/quests/weather_opinion_\" + year + \".csv\"\n","    df_main = pd.read_csv(link_main)\n","    df_opinion = pd.read_csv(link_opinion)\n","\n","    # Apply your merge, cleaning and fillna function on the current year\n","    df_year = mergeDF(df_main , df_opinion)\n","\n","    # Concat with the global DF\n","    dftotal = pd.concat([dftotal , df_year])\n","\n","\n","  # FILL IN MISSING VALUES\n","\n","\n","\n","  # ADD NEW COLUMNS\n","\n","\n","  # TRANSLATE OPINION\n","\n","  return NewDataFrame\n","\n"]},{"cell_type":"markdown","metadata":{"id":"851Xnj1M4_xd"},"source":["## Execution with a loop\n","You will execute your function in a 9 rounds loop, once per year, and agregate this in a new big DataFrame with 24 columns and lots of rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AE09BUYX5blp"},"outputs":[],"source":["# Each year between 2010 and 2018 (included) has 2 sources : weather_main_YYYY.csv and weather_opinion_YYYY.csv\n","\n","for year in range(2010, 2019):\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c5YtwMyw6BiW"},"source":["## Add 2019 to 2022"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTiJ3fAa6FEC"},"outputs":[],"source":["# Each year since 2019 (included) has 1 source : weatherYYYY.csv\n","# Chaque année depuis 2019 (inclus) a une seule source avec ce format de nom de fichier : weatherYYYY.csv\n","link2019 = \"https://raw.githubusercontent.com/WildCodeSchool/data-training-resources/main/quests/weather2019.csv\"\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a2DO5esW6Weh"},"source":["# Draw a scatterplot\n","With Date on X-Axis and Max Temperature on Y-axis\n","\n","The result must have 13 seasonality for the 13 summers/winters alternance.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7TKmTuSe1JP"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MnFjmbITe0mS"},"source":["\n","Find appropriate dataviz to answer to this question :\n","\n","Which year had the best weather? Are years very different or not ?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDneGuBle8cs"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1xAOk19Y0JRq0UTdoYGEV40-pgRWQbC2f","timestamp":1701678466078}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
