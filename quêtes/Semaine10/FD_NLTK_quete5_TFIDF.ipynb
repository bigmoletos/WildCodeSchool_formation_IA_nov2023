{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK Quetes 4  Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 Importe cet ensemble de données de tweets dans un DataFrame.\n",
    "* 2 Ne garde que les tweets positifs et négatifs (tu excluras donc les neutral). Quel est le pourcentage de tweets positifs/négatifs ?\n",
    "* 3 Copie la colonne text dans une Série X, et la colonne sentiment dans une Série y. Applique un train test split avec le random_state = 32 et un train_size de 0.75.\n",
    "* 4 Crée un modèle vectorizer avec scikit-learn en utilisant la méthode Countvectorizer. Entraîne ton modèle sur X_train, puis crée une matrice de features X_train_CV. Crée la matrice X_test_CV sans ré-entraîner le modèle. Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements.\n",
    "* 5 Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.966 pour le test d'entraînement, et * 0.877 pour l'ensemble de test.\n",
    "*  6 Étape bonus : essaye d'afficher 10 tweets qui ont été mal prédits (faux positifs ou faux négatifs). Aurais-tu fait mieux que l'algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import secrets\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "import plotly.io as pio\n",
    "from sklearn import tree\n",
    "from typing import Counter\n",
    "import plotly.express as px\n",
    "from fuzzywuzzy import fuzz\n",
    "from textblob import TextBlob\n",
    "from joblib import dump, load\n",
    "from bs4 import BeautifulSoup\n",
    "import category_encoders as ce\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from flask import Flask, request, render_template, session, url_for, redirect\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler, Normalizer,\n",
    "                                   PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler)\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, TargetEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "* Train test split\n",
    "* Fit et transform du Vectorizer sur le train set\n",
    "* Fit et score du classifier sur le train set\n",
    "* Transform (sans fit !) du Vectorizer sur le test set\n",
    "* Predict et score du classifier sur le test set\n",
    "\n",
    "### Paramètres du CountVectorizer\n",
    "\n",
    "A l'initialisation du CountVectorizer, tu peux spécifier quelques paramètres très intéressants. Citons notamment :\n",
    "\n",
    "* lowercase : permet de convertir tout le texte en minuscule\n",
    "* stop_words : permet de spécifier une liste de stopwords, qui ne généreront donc pas de colonnes dédiées\n",
    "* ngram_range : permet de spécifier si des bigrammes ou n-grammes doivent être pris en compte\n",
    "* max_features : limite le nombre de mots maximum, en ne prenant que les mots les plus fréquents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " matrice_creuse:\n",
      "3 \n",
      "\n",
      "\n",
      " liste_colonne_matrice_creuse:\n",
      "['negative' 'neutral' 'positive'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fichier_to_test = \"./le_bonheur.txt\"\n",
    "# print(f\"\\n fichier_to_test :\\n{fichier_to_test} \\n\")\n",
    "\n",
    "# # Lire le fichier\n",
    "# with open(fichier_to_test, \"r\", encoding=\"utf-8\") as file:\n",
    "#     texte = file.read()\n",
    "\n",
    "# # Tokenisation par phrase, liste les phrases dans le texte\n",
    "# token_phrases = nltk.sent_tokenize(texte)\n",
    "# print(f\"\\n token_phrases:\\n{token_phrases} \\n\")\n",
    "\n",
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "# on ne garde pas les lignes \\n\\n\n",
    "# df = df['sentiment']\n",
    "# tokens = df['tokens'].tolist()\n",
    "\n",
    "# # transformation en matrice creuse\n",
    "# vectorizer = CountVectorizer()\n",
    "# matrice_creuse=vectorizer.fit_transform(df['sentiment'])\n",
    "# print(f\"\\n matrice_creuse:\\n{matrice_creuse.shape[1]} \\n\")\n",
    "# # afficher la liste des noms de colonnes\n",
    "# liste_colonne_matrice_creuse=vectorizer.get_feature_names_out()\n",
    "# print(f\"\\n liste_colonne_matrice_creuse:\\n{liste_colonne_matrice_creuse} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filtrer les Tweets Positifs et Négatifs\n",
    "['negative' 'neutral' 'positive'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27480 entries, 0 to 27479\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "\n",
      " df.info():\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "print(f\"\\n df.info():\\n{df.info()} \\n\")\n",
    "# on ne garde pas les lignes \\n\\n\n",
    "#  Filtre les tewets positifs et negatifs\n",
    "df_filtered = df[df['sentiment'].isin(['positive', 'negative'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculer le Pourcentage de Tweets Positifs/Négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pourcentage_positif:\n",
      "0.52 \n",
      "\n",
      "\n",
      " pourcentage_negatif:\n",
      "0.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pourcentage_positif = (df_filtered['sentiment'] == 'positive').mean()\n",
    "pourcentage_negatif = (df_filtered['sentiment'] == 'negative').mean()\n",
    "print(f\"\\n pourcentage_positif:\\n{pourcentage_positif:.2f} \\n\")\n",
    "print(f\"\\n pourcentage_negatif:\\n{pourcentage_negatif:.2f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Préparation des Données pour le Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered['text']\n",
    "y = df_filtered['sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV.shape:\n",
      "(4091,) \n",
      "\n",
      "\n",
      " X_test_CV.size:\n",
      "<bound method Series.info of 5680      - no,  is buttfuck stupid. I`m just silly and...\n",
      "7661      get better omg i still dont believe that i di...\n",
      "2670     HollowbabesHere comes the utter shite #bgt <I ...\n",
      "5020      Thank You Clayton. Going to my favorite Greek...\n",
      "26962     I`m watching it at the moment  -sighs- and st...\n",
      "                               ...                        \n",
      "4062                                       I can`t take it\n",
      "4618      so where r u spinning now that the Hookah is ...\n",
      "18293              WHAT?! i was wanting to see that show!!\n",
      "16606                     Har vondt i ryggen My back hurts\n",
      "5223     Laying in bed with a book & some beautiful mus...\n",
      "Name: text, Length: 4091, dtype: object> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=32)\n",
    "# Vérifier la dimension de X_test_CV\n",
    "print(f\"\\n X_test_CV.shape:\\n{X_test.shape} \\n\")\n",
    "print(f\"\\n X_test_CV.size:\\n{X_test.info} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vectorisation\n",
    " Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(4091, 15806) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_CV = vectorizer.fit_transform(X_train)\n",
    "X_test_CV = vectorizer.transform(X_test)\n",
    "\n",
    "# print(f\"\\n X_train_CV:\\n{X_train_CV} \\n\")\n",
    "print(f\"\\n X_test_CV:\\n{X_test_CV.shape} \\n\")\n",
    "# print(f\"\\n X_train_CV:\\n{X_train_CV.toarray()} \\n\")\n",
    "# print(f\"\\n X_test_CV:\\n{X_test_CV.toarray()} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5 Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.966 pour le test d'entraînement, et * 0.877 pour l'ensemble de test.\n",
    "*  6 Étape bonus : essaye d'afficher 10 tweets qui ont été mal prédits (faux positifs ou faux négatifs). Aurais-tu fait mieux que l'algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Entraînement d'une Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy_train:0.966 \n",
      "\n",
      " accuracy_test:0.877 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_CV, y_train)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy_train = model.score(X_train_CV, y_train)\n",
    "accuracy_test = model.score(X_test_CV, y_test)\n",
    "\n",
    "print(f\"\\n accuracy_train:{accuracy_train:.3f} \")\n",
    "print(f\"\\n accuracy_test:{accuracy_test:.3f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étape Bonus : Affichage des Tweets Mal Prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2670     HollowbabesHere comes the utter shite #bgt <I ...\n",
      "18731     SUFFICATION NO BREATHING. It`s okay. There`ll...\n",
      "12054    i wanna vote for Miley Cyrus for the mtv movie...\n",
      "21823    I love music so much that i`ve gone through pa...\n",
      "18464    I can only message those who message me, if we...\n",
      "2975     wish I could feel no pain (8)  but it`s ok, at...\n",
      "3921                        so glad i`m not at uni anymore\n",
      "5198      You`re not here. I hope you`re still resting....\n",
      "467        you`re missing out, bb! i`m such a cereal nu...\n",
      "15215     have an amazing time with your mommas tomorro...\n",
      "Name: text, dtype: object\n",
      "\n",
      " rapport_performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.88      0.87      1935\n",
      "    positive       0.89      0.88      0.88      2156\n",
      "\n",
      "    accuracy                           0.88      4091\n",
      "   macro avg       0.88      0.88      0.88      4091\n",
      "weighted avg       0.88      0.88      0.88      4091\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_CV)\n",
    "# on selectionne toutes les instances de X_test  pour lesquelles les prédictions ne correspondent pas aux vraies valeurs. Cela vous donne un sous-ensemble de X_test contenant uniquement les instances mal prédites.\n",
    "mal_predits = X_test[(y_pred != y_test)]\n",
    "\n",
    "print(mal_predits.head(10))\n",
    "# performance globale du modele avec f1\n",
    "#   df.niveau1.value_counts()\n",
    "#  faire un tfidf\n",
    "# utiliser recall pour avoir le % de positifs bien predits\n",
    "# print(classification_report(y, knn.predict(X)))\n",
    "\n",
    "rapport_performance = classification_report(y_test, y_pred)\n",
    "print(f\"\\n rapport_performance:\\n{rapport_performance} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quete 5 NLP 5 : TfIdf\n",
    "Challenge - Tweetons !\n",
    "Nous allons effectuer les mêmes missions que dans la quête précédente, afin de comparer les deux méthodes (TfIdf Vectorizer et CountVectorizer). Pour rappel, les missions étaient les suivantes :\n",
    "\n",
    "* Importe cet ensemble de données de tweets dans un DataFrame.\n",
    "* Ne garde que les tweets positifs et négatifs (tu excluras donc les neutral). Quel est le pourcentage de tweets positifs/négatifs ?\n",
    "* Copie la colonne text dans une Série X, et la colonne sentiment dans une Série y. Applique un train test split avec le random_state = 32.\n",
    "* Crée un modèle vectorizer avec scikit-learn en utilisant la méthode TfidfVectorizer. \n",
    "* Entraîne ton modèle sur X_train, puis crée une matrice de features X_train_CV. \n",
    "* Crée la matrice X_test_CV sans ré-entraîner le modèle. Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements.\n",
    "* Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.932 pour le test d'entraînement, et 0.873 pour l'ensemble de test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importe cet ensemble de données de tweets dans un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27480 entries, 0 to 27479\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "\n",
      " df.info():\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "print(f\"\\n df.info():\\n{df.info()} \\n\")\n",
    "# on ne garde pas les lignes \\n\\n\n",
    "#  Filtre les tewets positifs et negatifs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ne garde que les tweets positifs et négatifs (tu excluras donc les neutral). Quel est le pourcentage de tweets positifs/négatifs ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pourcentage_positif:\n",
      "0.52 \n",
      "\n",
      "\n",
      " pourcentage_negatif:\n",
      "0.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df[df['sentiment'].isin(['positive', 'negative'])]\n",
    "pourcentage_positif = (df_filtered['sentiment'] == 'positive').mean()\n",
    "pourcentage_negatif = (df_filtered['sentiment'] == 'negative').mean()\n",
    "print(f\"\\n pourcentage_positif:\\n{pourcentage_positif:.2f} \\n\")\n",
    "print(f\"\\n pourcentage_negatif:\\n{pourcentage_negatif:.2f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copie la colonne text dans une Série X, et la colonne sentiment dans une Série y. Applique un train test split avec le random_state = 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered['text']\n",
    "y = df_filtered['sentiment']\n",
    "\n",
    "\n",
    "# Creation train et test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crée un modèle vectorizer avec scikit-learn en utilisant la méthode TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(3273, 16428) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Création du modèle TfidfVectorizer\n",
    "model_TFIDF = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Entraîne ton modèle sur X_train, puis crée une matrice de features X_train_CV. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(3273, 16428) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle sur X_train et création de la matrice de features X_train_CV\n",
    "X_train_CV = model_TFIDF.fit_transform(X_train)\n",
    "\n",
    "# Création de la matrice X_test_CV sans ré-entraîner le modèle\n",
    "X_test_CV = model_TFIDF.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crée la matrice X_test_CV sans ré-entraîner le modèle. Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(3273, 16428) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Création et entraînement du modèle de régression logistique\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_CV, y_train)\n",
    "\n",
    "# Prédictions sur les ensembles d'entraînement et de test\n",
    "y_train_pred = logistic_model.predict(X_train_CV)\n",
    "y_test_pred = logistic_model.predict(X_test_CV)\n",
    "# print(f\"\\n y_train_pred:\\n{y_train_pred} \\n\")\n",
    "# print(f\"\\n y_test_pred:\\n{y_test_pred} \\n\")\n",
    "print(f\"\\n X_test_CV:\\n{X_test_CV.shape} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.932 pour le test d'entraînement, et 0.873 pour l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_accuracy:0.930 \n",
      "\n",
      " test_accuracy:0.872 \n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'exactitude pour les deux ensembles\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\n train_accuracy:{train_accuracy:.3f} \")\n",
    "print(f\"\\n test_accuracy:{test_accuracy:.3f} \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
