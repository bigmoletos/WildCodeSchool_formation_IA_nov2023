{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK Quetes 4  Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 Importe cet ensemble de données de tweets dans un DataFrame.\n",
    "* 2 Ne garde que les tweets positifs et négatifs (tu excluras donc les neutral). Quel est le pourcentage de tweets positifs/négatifs ?\n",
    "* 3 Copie la colonne text dans une Série X, et la colonne sentiment dans une Série y. Applique un train test split avec le random_state = 32 et un train_size de 0.75.\n",
    "* 4 Crée un modèle vectorizer avec scikit-learn en utilisant la méthode Countvectorizer. Entraîne ton modèle sur X_train, puis crée une matrice de features X_train_CV. Crée la matrice X_test_CV sans ré-entraîner le modèle. Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements.\n",
    "* 5 Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.966 pour le test d'entraînement, et * 0.877 pour l'ensemble de test.\n",
    "*  6 Étape bonus : essaye d'afficher 10 tweets qui ont été mal prédits (faux positifs ou faux négatifs). Aurais-tu fait mieux que l'algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import secrets\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "import plotly.io as pio\n",
    "from sklearn import tree\n",
    "from typing import Counter\n",
    "import plotly.express as px\n",
    "from fuzzywuzzy import fuzz\n",
    "from textblob import TextBlob\n",
    "from joblib import dump, load\n",
    "from bs4 import BeautifulSoup\n",
    "import category_encoders as ce\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from flask import Flask, request, render_template, session, url_for, redirect\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler, Normalizer,\n",
    "                                   PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler)\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, TargetEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "* Train test split\n",
    "* Fit et transform du Vectorizer sur le train set\n",
    "* Fit et score du classifier sur le train set\n",
    "* Transform (sans fit !) du Vectorizer sur le test set\n",
    "* Predict et score du classifier sur le test set\n",
    "\n",
    "### Paramètres du CountVectorizer\n",
    "\n",
    "A l'initialisation du CountVectorizer, tu peux spécifier quelques paramètres très intéressants. Citons notamment :\n",
    "\n",
    "* lowercase : permet de convertir tout le texte en minuscule\n",
    "* stop_words : permet de spécifier une liste de stopwords, qui ne généreront donc pas de colonnes dédiées\n",
    "* ngram_range : permet de spécifier si des bigrammes ou n-grammes doivent être pris en compte\n",
    "* max_features : limite le nombre de mots maximum, en ne prenant que les mots les plus fréquents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " matrice_creuse:\n",
      "3 \n",
      "\n",
      "\n",
      " liste_colonne_matrice_creuse:\n",
      "['negative' 'neutral' 'positive'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fichier_to_test = \"./le_bonheur.txt\"\n",
    "# print(f\"\\n fichier_to_test :\\n{fichier_to_test} \\n\")\n",
    "\n",
    "# # Lire le fichier\n",
    "# with open(fichier_to_test, \"r\", encoding=\"utf-8\") as file:\n",
    "#     texte = file.read()\n",
    "\n",
    "# # Tokenisation par phrase, liste les phrases dans le texte\n",
    "# token_phrases = nltk.sent_tokenize(texte)\n",
    "# print(f\"\\n token_phrases:\\n{token_phrases} \\n\")\n",
    "\n",
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "# on ne garde pas les lignes \\n\\n\n",
    "# df = df['sentiment']\n",
    "# tokens = df['tokens'].tolist()\n",
    "\n",
    "# # transformation en matrice creuse\n",
    "# vectorizer = CountVectorizer()\n",
    "# matrice_creuse=vectorizer.fit_transform(df['sentiment'])\n",
    "# print(f\"\\n matrice_creuse:\\n{matrice_creuse.shape[1]} \\n\")\n",
    "# # afficher la liste des noms de colonnes\n",
    "# liste_colonne_matrice_creuse=vectorizer.get_feature_names_out()\n",
    "# print(f\"\\n liste_colonne_matrice_creuse:\\n{liste_colonne_matrice_creuse} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filtrer les Tweets Positifs et Négatifs\n",
    "['negative' 'neutral' 'positive'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27480 entries, 0 to 27479\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "\n",
      " df.info():\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "print(f\"\\n df.info():\\n{df.info()} \\n\")\n",
    "# on ne garde pas les lignes \\n\\n\n",
    "#  Filtre les tewets positifs et negatifs\n",
    "df_filtered = df[df['sentiment'].isin(['positive', 'negative'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculer le Pourcentage de Tweets Positifs/Négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pourcentage_positif:\n",
      "0.52 \n",
      "\n",
      "\n",
      " pourcentage_negatif:\n",
      "0.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pourcentage_positif = (df_filtered['sentiment'] == 'positive').mean()\n",
    "pourcentage_negatif = (df_filtered['sentiment'] == 'negative').mean()\n",
    "print(f\"\\n pourcentage_positif:\\n{pourcentage_positif:.2f} \\n\")\n",
    "print(f\"\\n pourcentage_negatif:\\n{pourcentage_negatif:.2f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Préparation des Données pour le Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered['text']\n",
    "y = df_filtered['sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV.shape:\n",
      "(4091,) \n",
      "\n",
      "\n",
      " X_test_CV.size:\n",
      "<bound method Series.info of 5680      - no,  is buttfuck stupid. I`m just silly and...\n",
      "7661      get better omg i still dont believe that i di...\n",
      "2670     HollowbabesHere comes the utter shite #bgt <I ...\n",
      "5020      Thank You Clayton. Going to my favorite Greek...\n",
      "26962     I`m watching it at the moment  -sighs- and st...\n",
      "                               ...                        \n",
      "4062                                       I can`t take it\n",
      "4618      so where r u spinning now that the Hookah is ...\n",
      "18293              WHAT?! i was wanting to see that show!!\n",
      "16606                     Har vondt i ryggen My back hurts\n",
      "5223     Laying in bed with a book & some beautiful mus...\n",
      "Name: text, Length: 4091, dtype: object> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=32)\n",
    "# Vérifier la dimension de X_test_CV\n",
    "print(f\"\\n X_test_CV.shape:\\n{X_test.shape} \\n\")\n",
    "print(f\"\\n X_test_CV.size:\\n{X_test.info} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vectorisation\n",
    " Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(4091, 15806) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_CV = vectorizer.fit_transform(X_train)\n",
    "X_test_CV = vectorizer.transform(X_test)\n",
    "\n",
    "# print(f\"\\n X_train_CV:\\n{X_train_CV} \\n\")\n",
    "print(f\"\\n X_test_CV:\\n{X_test_CV.shape} \\n\")\n",
    "# print(f\"\\n X_train_CV:\\n{X_train_CV.toarray()} \\n\")\n",
    "# print(f\"\\n X_test_CV:\\n{X_test_CV.toarray()} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5 Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.966 pour le test d'entraînement, et * 0.877 pour l'ensemble de test.\n",
    "*  6 Étape bonus : essaye d'afficher 10 tweets qui ont été mal prédits (faux positifs ou faux négatifs). Aurais-tu fait mieux que l'algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Entraînement d'une Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy_train:0.966 \n",
      "\n",
      " accuracy_test:0.877 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_CV, y_train)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy_train = model.score(X_train_CV, y_train)\n",
    "accuracy_test = model.score(X_test_CV, y_test)\n",
    "\n",
    "print(f\"\\n accuracy_train:{accuracy_train:.3f} \")\n",
    "print(f\"\\n accuracy_test:{accuracy_test:.3f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étape Bonus : Affichage des Tweets Mal Prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2670     HollowbabesHere comes the utter shite #bgt <I ...\n",
      "18731     SUFFICATION NO BREATHING. It`s okay. There`ll...\n",
      "12054    i wanna vote for Miley Cyrus for the mtv movie...\n",
      "21823    I love music so much that i`ve gone through pa...\n",
      "18464    I can only message those who message me, if we...\n",
      "2975     wish I could feel no pain (8)  but it`s ok, at...\n",
      "3921                        so glad i`m not at uni anymore\n",
      "5198      You`re not here. I hope you`re still resting....\n",
      "467        you`re missing out, bb! i`m such a cereal nu...\n",
      "15215     have an amazing time with your mommas tomorro...\n",
      "Name: text, dtype: object\n",
      "\n",
      " rapport_performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.88      0.87      1935\n",
      "    positive       0.89      0.88      0.88      2156\n",
      "\n",
      "    accuracy                           0.88      4091\n",
      "   macro avg       0.88      0.88      0.88      4091\n",
      "weighted avg       0.88      0.88      0.88      4091\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_CV)\n",
    "# on selectionne toutes les instances de X_test  pour lesquelles les prédictions ne correspondent pas aux vraies valeurs. Cela vous donne un sous-ensemble de X_test contenant uniquement les instances mal prédites.\n",
    "mal_predits = X_test[(y_pred != y_test)]\n",
    "\n",
    "print(mal_predits.head(10))\n",
    "# performance globale du modele avec f1\n",
    "#   df.niveau1.value_counts()\n",
    "#  faire un tfidf\n",
    "# utiliser recall pour avoir le % de positifs bien predits\n",
    "# print(classification_report(y, knn.predict(X)))\n",
    "\n",
    "rapport_performance = classification_report(y_test, y_pred)\n",
    "print(f\"\\n rapport_performance:\\n{rapport_performance} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quete 5 NLP 5 : TfIdf\n",
    "Challenge - Tweetons !\n",
    "Nous allons effectuer les mêmes missions que dans la quête précédente, afin de comparer les deux méthodes (TfIdf Vectorizer et CountVectorizer). Pour rappel, les missions étaient les suivantes :\n",
    "\n",
    "* Importe cet ensemble de données de tweets dans un DataFrame.\n",
    "* Ne garde que les tweets positifs et négatifs (tu excluras donc les neutral). Quel est le pourcentage de tweets positifs/négatifs ?\n",
    "* Copie la colonne text dans une Série X, et la colonne sentiment dans une Série y. Applique un train test split avec le random_state = 32.\n",
    "* Crée un modèle vectorizer avec scikit-learn en utilisant la méthode TfidfVectorizer. \n",
    "* Entraîne ton modèle sur X_train, puis crée une matrice de features X_train_CV. \n",
    "* Crée la matrice X_test_CV sans ré-entraîner le modèle. Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements.\n",
    "* Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.932 pour le test d'entraînement, et 0.873 pour l'ensemble de test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importe cet ensemble de données de tweets dans un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27480 entries, 0 to 27479\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "\n",
      " df.info():\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "print(f\"\\n df.info():\\n{df.info()} \\n\")\n",
    "# on ne garde pas les lignes \\n\\n\n",
    "#  Filtre les tewets positifs et negatifs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ne garde que les tweets positifs et négatifs (tu excluras donc les neutral). Quel est le pourcentage de tweets positifs/négatifs ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pourcentage_positif:\n",
      "0.52 \n",
      "\n",
      "\n",
      " pourcentage_negatif:\n",
      "0.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df[df['sentiment'].isin(['positive', 'negative'])]\n",
    "pourcentage_positif = (df_filtered['sentiment'] == 'positive').mean()\n",
    "pourcentage_negatif = (df_filtered['sentiment'] == 'negative').mean()\n",
    "print(f\"\\n pourcentage_positif:\\n{pourcentage_positif:.2f} \\n\")\n",
    "print(f\"\\n pourcentage_negatif:\\n{pourcentage_negatif:.2f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copie la colonne text dans une Série X, et la colonne sentiment dans une Série y. Applique un train test split avec le random_state = 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered['text']\n",
    "y = df_filtered['sentiment']\n",
    "\n",
    "\n",
    "# Creation train et test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crée un modèle vectorizer avec scikit-learn en utilisant la méthode TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(3273, 16428) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Création du modèle TfidfVectorizer\n",
    "model_TFIDF = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Entraîne ton modèle sur X_train, puis crée une matrice de features X_train_CV. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(3273, 16428) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle sur X_train et création de la matrice de features X_train_CV\n",
    "X_train_CV = model_TFIDF.fit_transform(X_train)\n",
    "\n",
    "# Création de la matrice X_test_CV sans ré-entraîner le modèle\n",
    "X_test_CV = model_TFIDF.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crée la matrice X_test_CV sans ré-entraîner le modèle. Le format de la matrice X_test_CV doit être 4091x15806 avec 44633 stored elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(3273, 16428) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Création et entraînement du modèle de régression logistique\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_CV, y_train)\n",
    "\n",
    "# Prédictions sur les ensembles d'entraînement et de test\n",
    "y_train_pred = logistic_model.predict(X_train_CV)\n",
    "y_test_pred = logistic_model.predict(X_test_CV)\n",
    "# print(f\"\\n y_train_pred:\\n{y_train_pred} \\n\")\n",
    "# print(f\"\\n y_test_pred:\\n{y_test_pred} \\n\")\n",
    "print(f\"\\n X_test_CV:\\n{X_test_CV.shape} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîne maintenant une régression logistique avec les paramètres par défaut. Tu devrais obtenir les résultats suivants : 0.932 pour le test d'entraînement, et 0.873 pour l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_accuracy:0.930 \n",
      "\n",
      " test_accuracy:0.872 \n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'exactitude pour les deux ensembles\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\n train_accuracy:{train_accuracy:.3f} \")\n",
    "print(f\"\\n test_accuracy:{test_accuracy:.3f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 6: Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importation du dataset de tweets dans un DataFrame\n",
    "\n",
    "Importez le dataset de tweets dans un DataFrame. Conservez uniquement les tweets positifs et négatifs (excluez les neutres). Calculez le pourcentage de tweets positifs/négatifs.\n",
    "\n",
    "* Création de la fonction `clean`\n",
    "\n",
    "- Créez une fonction que vous appellerez `clean`. Cette fonction prend en paramètre une phrase (un texte `str`) et retourne un texte (`str`) de tokens après application d'un stemmer ou d'un lemmatizer, séparés par des espaces.\n",
    "\n",
    "- Vous pouvez tester votre fonction avec cette phrase, elle doit retourner quelque chose ressemblant à ce résultat :\n",
    "\n",
    "``clean(\"You are better when I am well.\")\n",
    "\"you be well when I be well .\"``\n",
    "\n",
    "* Suppression de la ponctuation et des stopwords\n",
    "* Récupérez la liste des stopwords anglais depuis NLTK, et copiez-la dans une liste stopwordsenglish. Complétez votre fonction clean pour qu’elle supprime la ponctuation et les stopwords.\n",
    "\n",
    "* Appliquez cette fonction clean à la colonne text de votre DataFrame. \n",
    "\n",
    "* Stockez le résultat dans une nouvelle colonne clean du DataFrame. (Le traitement peut durer 2 ou 3 minutes)\n",
    "- Votre DataFrame doit maintenant ressembler à celui-ci (moins les pronoms, la ponctuation et peut-être d’autres mots en fonction des stopwords que vous avez nettoyés) :\n",
    " !DataFrame\n",
    "\n",
    "* Préparation des données pour l’entraînement du modèle\n",
    "- Copiez la colonne clean dans une Serie X, et la colonne sentiment dans une Serie y. \n",
    "- Appliquez un train-test split avec la taille du jeu d’entrainement à 0.75 avec le random_state = 32.\n",
    "\n",
    "* Entraînement des modèles de classification\n",
    "* Appliquez un CountVectorizer et entraînez des modèles de classification.\n",
    "\n",
    "* Appliquez un TfidfVectorizer et entraînez des modèles de classification.\n",
    " \n",
    "* Comparez les scores, quels paramètres permettent d’avoir les meilleurs scores ?\n",
    " \n",
    "* Bonus : Amélioration du modèle. Maintenant, c’est à vous d’améliorer votre modèle :\n",
    "* En cherchant des paramètres de modèles : par gridsearch et crossvalidation par exemple ;\n",
    "* En changeant la préparation du texte : par exemple certaines ponctuations peuvent aider le modèle, le point d’exclamation notamment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation du dataset de tweets dans un DataFrame\n",
    "\n",
    "- Importez le dataset de tweets dans un DataFrame. Conservez uniquement les tweets positifs et négatifs (excluez les neutres). Calculez le pourcentage de tweets positifs/négatifs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27480 entries, 0 to 27479\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "\n",
      " df.info():\n",
      "None \n",
      "\n",
      "\n",
      " df_filtered:\n",
      "       textID                                            text selected_text  \\\n",
      "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!      Sooo SAD   \n",
      "2  088c60f138                       my boss is bullying me...   bullying me   \n",
      "\n",
      "  sentiment  \n",
      "1  negative  \n",
      "2  negative   \n",
      "\n",
      "\n",
      " pourcentage_positif:\n",
      "0.52 \n",
      "\n",
      "\n",
      " pourcentage_negatif:\n",
      "0.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger le DataFrame à partir du fichier CSV\n",
    "df = pd.read_csv(\"tweets_train.csv\")\n",
    "print(f\"\\n df.info():\\n{df.info()} \\n\")\n",
    "df_filtered = df[df['sentiment'].isin(['positive', 'negative'])]\n",
    "print(f\"\\n df_filtered:\\n{df_filtered.head(2)} \\n\")\n",
    "pourcentage_positif = (df_filtered['sentiment'] == 'positive').mean()\n",
    "pourcentage_negatif = (df_filtered['sentiment'] == 'negative').mean()\n",
    "print(f\"\\n pourcentage_positif:\\n{pourcentage_positif:.2f} \\n\")\n",
    "print(f\"\\n pourcentage_negatif:\\n{pourcentage_negatif:.2f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la fonction `clean`\n",
    "\n",
    "- Créez une fonction que vous appellerez `clean`. Cette fonction prend en paramètre une phrase (un texte `str`) et retourne un texte (`str`) de tokens après application d'un stemmer ou d'un lemmatizer, séparés par des espaces.\n",
    "\n",
    "- Vous pouvez tester votre fonction avec cette phrase, elle doit retourner quelque chose ressemblant à ce résultat :\n",
    "\n",
    "``clean(\"You are better when I am well.\")\n",
    "\"you be well when I be well .\"``\n",
    "\n",
    "### Suppression de la ponctuation et des stopwords\n",
    "### Récupérez la liste des stopwords anglais depuis NLTK, et copiez-la dans une liste stopwordsenglish. Complétez votre fonction clean pour qu’elle supprime la ponctuation et les stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean(text):\n",
    "    # Initialisation des modèles de stemming et lemmatization\n",
    "    stemmer_snowball = SnowballStemmer('english')\n",
    "    lemmatizer_wordnet = WordNetLemmatizer()\n",
    "    lemmatizer_TextBlob = TextBlob(text)\n",
    "\n",
    "    # Si le texte est vide, retournez des listes vides sinon on ne pourra pas créer les colonnes pd.Series([stemmer_snowball_tokens, lemmatizer_wordnet_tokens, lemmatized_TextBlob_tokens])\n",
    "    if not  isinstance(text, str) or not text:\n",
    "        return [], [], []\n",
    "\n",
    "    # print(f\" stemmer_snowball:{stemmer_snowball} \")\n",
    "    # print(f\" lemmatizer_worldnet:{lemmatizer_worldnet} \")\n",
    "    # print(f\" lemmatizer_TextBlob :{lemmatizer_TextBlob} \")\n",
    "\n",
    "    # Si le texte est vide, retournez des listes vides sinon on ne pourra pas créer les colonnes pd.Series([stemmer_snowball_tokens, lemmatizer_worldnet_tokens, lemmatized_TextBlob_tokens])\n",
    "    print(f\" text:{text} \")\n",
    "\n",
    "    # Chargement des stopwords anglais\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenisation\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Suppression des caractères spéciaux\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "\n",
    "    # Stemming avec SnowballStemmer\n",
    "    stemmer_snowball_tokens = [stemmer_snowball.stem(\n",
    "        token) for token in tokens if token not in stopwords_english and token not in string.punctuation]\n",
    "\n",
    "    # Lemmatization with WordNetLemmatizer\n",
    "    lemmatizer_worldnet_tokens = [lemmatizer_wordnet.lemmatize(token) for token in tokens if token not in stopwords_english and token not in string.punctuation]\n",
    "\n",
    "    # # Lemmatization with TextBlob\n",
    "    # lemmatized_TextBlob_tokens = [lemmatizer_TextBlob.lemmatize(token) for token in tokens if token not in stopwords_english and token not in string.punctuation]\n",
    "\n",
    "    #   # Lemmatisation avec TextBlob\n",
    "    lemmatized_TextBlob_tokens = [Word(word).lemmatize().lower() for word in lemmatizer_TextBlob.words if word.lower(\n",
    "    ) not in stopwords_english and word not in string.punctuation]\n",
    "    # print(f\"\\n stemmer_snowball_tokens:{stemmer_snowball_tokens} \")\n",
    "    # print(f\"lemmatizer_worldnet_tokens :{lemmatizer_worldnet_tokens} \")\n",
    "    # print(f\" lemmatized_TextBlob_tokens:{lemmatized_TextBlob_tokens} \")\n",
    "\n",
    "    return stemmer_snowball_tokens, lemmatizer_worldnet_tokens, lemmatized_TextBlob_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquez cette fonction clean à la colonne text de votre DataFrame. \n",
    "- Stockez le résultat dans une nouvelle colonne clean du DataFrame. (Le traitement peut durer 2 ou 3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  copie du DataFrame pour éviter le SettingWithCopyWarning\n",
    "df_filtered_copy = df_filtered.copy()\n",
    "\n",
    "# Appliquer la fonction clean et séparer les résultats\n",
    "results = df_filtered_copy['text'].apply(clean)\n",
    "\n",
    "# Créer de nouvelles colonnes pour chaque résultat dans la copie du DataFrame\n",
    "df_filtered_copy['clean_snowball'] = results.apply(lambda x: x[0])\n",
    "df_filtered_copy['clean_wordnet'] = results.apply(lambda x: x[1])\n",
    "df_filtered_copy['clean_textblob'] = results.apply(lambda x: x[2])\n",
    "\n",
    "# Déokéniser chaque colonne de facon à avoir des chaines de caractéres et non des listes\n",
    "df_filtered_copy['clean_snowball'] = df_filtered_copy['clean_snowball'].apply(lambda tokens: ' '.join(tokens))\n",
    "df_filtered_copy['clean_wordnet'] = df_filtered_copy['clean_wordnet'].apply(lambda tokens: ' '.join(tokens))\n",
    "df_filtered_copy['clean_textblob'] = df_filtered_copy['clean_textblob'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Afficher les premières lignes pour vérifier\n",
    "print(df_filtered_copy[['clean_snowball', 'clean_wordnet', 'clean_textblob']].head())\n",
    "\n",
    "# Maintenant, df_filtered_copy contient les nouvelles colonnes sans warnings\n",
    "df_filtered_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données pour l’entraînement du modèle\n",
    "- Copiez la colonne clean dans une Serie X, et la colonne sentiment dans une Serie y. \n",
    "- Appliquez un train-test split avec la taille du jeu d’entrainement à 0.75 avec le random_state = 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                  sooo sad miss san diego\n",
       "2                                           boss bulli ...\n",
       "3                                      interview leav alon\n",
       "4                            son put releas alreadi bought\n",
       "6                              2am feed babi fun smile coo\n",
       "                               ...                        \n",
       "27474                                       enjoy ur night\n",
       "27475    wish could come see u denver husband lost job ...\n",
       "27476    wonder rake client made clear .net forc dev le...\n",
       "27477    yay good enjoy break probabl need hectic weeke...\n",
       "27478                                                worth\n",
       "Name: clean_snowball, Length: 16363, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copie de la colonne clean dans une Serie X, et de la colonne sentiment dans une Serie y\n",
    "X = df_filtered_copy['clean_snowball']\n",
    "y = df_filtered_copy['sentiment']\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application d'un train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                  sooo sad miss san diego\n",
       "2                                           boss bulli ...\n",
       "3                                      interview leav alon\n",
       "4                            son put releas alreadi bought\n",
       "6                              2am feed babi fun smile coo\n",
       "                               ...                        \n",
       "27474                                       enjoy ur night\n",
       "27475    wish could come see u denver husband lost job ...\n",
       "27476    wonder rake client made clear .net forc dev le...\n",
       "27477    yay good enjoy break probabl need hectic weeke...\n",
       "27478                                                worth\n",
       "Name: clean_snowball, Length: 16363, dtype: object"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement des modèles de classification\n",
    "### Appliquez un CountVectorizer et entraînez des modèles de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(4091, 12941) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Conversion du texte en matrice creuse pour avoir un bag_of_word\n",
    "model_vectorizer = CountVectorizer()\n",
    "X_train_CV = model_vectorizer.fit_transform(X_train)\n",
    "X_test_CV = model_vectorizer.transform(X_test)\n",
    "\n",
    "# print(f\"\\n X_train_CV:\\n{X_train_CV} \\n\")\n",
    "print(f\"\\n X_test_CV:\\n{X_test_CV.shape} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquez un TfidfVectorizer et entraînez des modèles de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " X_test_CV:\n",
      "(4091, 12941) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Création du modèle TfidfVectorizer\n",
    "model_TFIDF = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# Entraînement du modèle sur X_train et création de la matrice de features X_train_CV\n",
    "X_train_tfidf= model_TFIDF.fit_transform(X_train)\n",
    "\n",
    "# Création de la matrice X_test_CV sans ré-entraîner le modèle\n",
    "X_test_tfidf= model_TFIDF.transform(X_test)\n",
    "print(f\"\\n X_test_CV:\\n{X_test_tfidf.shape} \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainnement des modéles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#  Entrainnement sur CountVectorizer\n",
    "model_vectorizer = LogisticRegression(max_iter=1000)\n",
    "model_vectorizer.fit(X_train_CV, y_train)\n",
    "\n",
    "\n",
    "#  Entrainnement sur TFIDF\n",
    "model_TFIDF = LogisticRegression(max_iter=1000)\n",
    "model_TFIDF.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparez les scores, quels paramètres permettent d’avoir les meilleurs scores ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy_train CountVectorizer :0.95 accuracy_test: 0.87 \n",
      " accuracy_train TFIDF:0.93 accuracy_test:0.87 \n"
     ]
    }
   ],
   "source": [
    "# Évaluer le modèle Bag of word avec CountVectorizer\n",
    "accuracy_train = model_vectorizer.score(X_train_CV, y_train)\n",
    "accuracy_test = model_vectorizer.score(X_test_CV, y_test)\n",
    "print(f\" accuracy_train CountVectorizer :{accuracy_train:.2f} accuracy_test: {accuracy_test:.2f} \")\n",
    "# Évaluer le modèle TFIDF\n",
    "accuracy_train = model_TFIDF.score(X_train_tfidf, y_train)\n",
    "accuracy_test = model_TFIDF.score(X_test_tfidf, y_test)\n",
    "print(f\" accuracy_train TFIDF:{accuracy_train:.2f} accuracy_test:{accuracy_test:.2f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus : Amélioration du modèle. Maintenant, c’est à vous d’améliorer votre modèle :\n",
    "### En cherchant des paramètres de modèles : par gridsearch et crossvalidation par exemple ;\n",
    "### En changeant la préparation du texte : par exemple certaines ponctuations peuvent aider le modèle, le point d’exclamation notamment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
